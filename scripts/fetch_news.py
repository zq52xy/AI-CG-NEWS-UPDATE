#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
[INPUT]: ä¾èµ– httpx, feedparser, beautifulsoup4 è¿›è¡Œç½‘ç»œè¯·æ±‚
[OUTPUT]: å¯¹å¤–æä¾› CLI æ¥å£ï¼Œç”Ÿæˆ Markdown æ ¼å¼çš„æ–°é—»æŠ¥å‘Š
[POS]: ai-cg-news-aggregator Skill çš„æ ¸å¿ƒæ‰§è¡Œè„šæœ¬
[PROTOCOL]: å˜æ›´æ—¶æ›´æ–°æ­¤å¤´éƒ¨ï¼Œç„¶åæ£€æŸ¥ CLAUDE.md
"""

# ============================================================================
#                      AI & CG æ–°é—»èšåˆè„šæœ¬
# ============================================================================

import argparse
import datetime
import html
import json
import os
import re
import sys
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, field

# ============================================================================
#                           æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

@dataclass
class NewsItem:
    """å•æ¡æ–°é—»æ•°æ®"""
    title: str
    url: str
    source: str
    category: str
    score: int = 0
    comments: int = 0
    authors: str = ""
    summary: str = ""
    date: str = ""
    image_url: str = ""  # æ–°å¢å›¾ç‰‡é“¾æ¥å­—æ®µ


# ============================================================================
#                          æ–‡æœ¬æ¸…æ´—å·¥å…·
# ============================================================================

def sanitize_html_text(text: str, max_length: Optional[int] = None) -> str:
    """
    æ¸…ç† HTML ç‰‡æ®µï¼Œä¿è¯è¾“å‡ºä¸ºçº¯æ–‡æœ¬ï¼Œé¿å…ç ´åå¡ç‰‡å¸ƒå±€
    - å…ˆè§£ç  HTML å®ä½“
    - å†ä½¿ç”¨è§£æå™¨ç§»é™¤æ®‹ç¼ºæ ‡ç­¾
    - åˆå¹¶å¤šä½™ç©ºç™½
    """
    if not text:
        return ""

    from bs4 import BeautifulSoup

    decoded = html.unescape(text)
    cleaned = BeautifulSoup(decoded, "html.parser").get_text(" ", strip=True)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()

    if max_length and len(cleaned) > max_length:
        return cleaned[:max_length - 3] + '...'
    return cleaned
    tags: list = field(default_factory=list)  # è‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾
    extra: dict = field(default_factory=dict)



# ============================================================================
#                           é…ç½®åŠ è½½æ¨¡å—
# ============================================================================

# é€šç”¨ User-Agentï¼Œé¿å…è¢«åçˆ¬è™«æ‹¦æˆª
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

GLOBAL_CONFIG = {}

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    global GLOBAL_CONFIG
    script_dir = Path(__file__).parent.resolve()
    config_path = script_dir.parent / 'config.json'
    
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                GLOBAL_CONFIG = json.load(f)
            print(f"[INFO] å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        except Exception as e:
            print(f"[WARN] é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    else:
        print(f"[INFO] æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤å†…ç½®è§„åˆ™")


def generate_tags(title: str, summary: str, source: str = "", category: str = "") -> list[str]:
    """
    æ ¹æ®å…³é”®è¯è§„åˆ™ä¸ºæ–°é—»ç”Ÿæˆæ ‡ç­¾
    
    Args:
        title: æ–°é—»æ ‡é¢˜
        summary: æ–°é—»æ‘˜è¦
        source: æ–°é—»æ¥æºï¼ˆå¦‚ arXiv, GitHubï¼‰
        category: æ–°é—»åˆ†ç±»ï¼ˆå¦‚ cs.CV, r/blenderï¼‰
    
    Returns:
        æ ‡ç­¾åˆ—è¡¨
    """
    tags = set()
    
    # åˆå¹¶æ–‡æœ¬ç”¨äºåŒ¹é…
    text = f"{title} {summary} {category}".lower()
    
    # ä»é…ç½®è·å–æ ‡ç­¾è§„åˆ™
    tag_rules = GLOBAL_CONFIG.get('tag_rules', {})
    
    # è§„åˆ™åŒ¹é…
    for tag_name, keywords in tag_rules.items():
        for keyword in keywords:
            if keyword.lower() in text:
                tags.add(tag_name)
                break  # åŒ¹é…åˆ°ä¸€ä¸ªå…³é”®è¯å³å¯ï¼Œé¿å…é‡å¤
    
    # åŸºäºæ¥æºæ·»åŠ é»˜è®¤æ ‡ç­¾
    source_tags = {
        'arXiv': 'è®ºæ–‡/ç ”ç©¶',
        'GitHub': 'å¼€æºé¡¹ç›®',
        'HackerNews': 'è¡Œä¸šåŠ¨æ€',
        'Reddit': 'ç¤¾åŒºè®¨è®º',
        'Reddit-CG': 'CGå›¾å½¢å­¦',
        'Official': 'å®˜æ–¹èµ„è®¯',
        'ProductHunt': 'äº§å“å‘å¸ƒ',
        'HuggingFace': 'æœºå™¨å­¦ä¹ ',
        'Skills.sh': 'Agent Skill',
    }
    if source in source_tags:
        tags.add(source_tags[source])
    
    return list(tags)

# ============================================================================
#                           arXiv æŠ“å–æ¨¡å—
# ============================================================================

def fetch_arxiv(categories: list[str], days: int = 1, max_results: int = 30) -> list[NewsItem]:
    """
    ä» arXiv è·å–æœ€æ–°è®ºæ–‡ï¼ˆä½¿ç”¨ Atom APIï¼Œæ¯” RSS æ›´ç¨³å®šï¼‰
    """
    try:
        import feedparser
    except ImportError:
        print("[ERROR] è¯·å®‰è£… feedparser: pip install feedparser")
        return []
    
    items = []
    
    # å…³é”®è¯è¿‡æ»¤ï¼ˆä½¿ç”¨é…ç½®ï¼‰
    config = GLOBAL_CONFIG.get('sources', {}).get('arxiv', {})
    keywords = config.get('keywords', [
        'neural rendering', 'diffusion', 'transformer', 'ray tracing',
        'real-time', 'GPU', '3D', 'generative', 'NeRF', 'Gaussian',
        'language model', 'vision', 'multimodal', 'embodied'
    ])
    
    per_cat_limit = max(10, max_results // len(categories))
    
    for cat in categories:
        # ä½¿ç”¨ Atom API
        url = f"https://export.arxiv.org/api/query?search_query=cat:{cat}&start=0&max_results={per_cat_limit}&sortBy=submittedDate&sortOrder=descending"
        print(f"[INFO] æ­£åœ¨è·å– arXiv {cat}...")
        
        try:
            feed = feedparser.parse(url)
            
            if not feed.entries:
                rss_url = f"https://export.arxiv.org/rss/{cat}"
                print(f"[INFO] Atom API æ— ç»“æœï¼Œå°è¯• RSS: {cat}")
                feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:per_cat_limit]:
                # è¿‡æ»¤é»‘åå•
                if is_blacklisted(entry.title) or is_blacklisted(entry.get('summary', '')):
                    continue

                title = re.sub(r'^\([^)]+\)\s*', '', entry.title).strip()
                
                authors = entry.get('author', entry.get('authors', 'Unknown'))
                if isinstance(authors, list):
                    authors = ', '.join([a.get('name', str(a)) for a in authors[:3]])
                
                summary = sanitize_html_text(entry.get('summary', ''), max_length=200)
                
                link = entry.get('link', '')
                if isinstance(entry.get('links'), list) and entry.links:
                    for l in entry.links:
                        if l.get('type') == 'application/pdf':
                            link = l.get('href', link)
                            break
                
                score = sum(1 for kw in keywords if kw.lower() in title.lower() or kw.lower() in summary.lower())
                
                items.append(NewsItem(
                    title=title,
                    url=link,
                    source='arXiv',
                    category=cat,
                    authors=authors if isinstance(authors, str) else str(authors),
                    summary=summary,
                    score=score,
                    date=entry.get('published', '')
                ))
                
        except Exception as e:
            print(f"[WARN] è·å– {cat} å¤±è´¥: {e}")
    
    items.sort(key=lambda x: x.score, reverse=True)
    return items[:max_results]


def fetch_github(topics: list[str], language: str = "", since: str = "daily") -> list[NewsItem]:
    """
    ä» GitHub Trending è·å–çƒ­é—¨é¡¹ç›®
    
    Args:
        topics: è¯é¢˜æ ‡ç­¾åˆ—è¡¨
        language: ç¼–ç¨‹è¯­è¨€ç­›é€‰
        since: æ—¶é—´èŒƒå›´ (daily/weekly/monthly)
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import httpx
        from bs4 import BeautifulSoup
    except ImportError:
        print("[ERROR] è¯·å®‰è£…ä¾èµ–: pip install httpx beautifulsoup4")
        return []
    
    items = []
    url = f"https://github.com/trending/{language}?since={since}"
    
    print(f"[INFO] æ­£åœ¨è·å– GitHub Trending ({language or 'all'})...")
    
    try:
        headers = {
            'User-Agent': USER_AGENT
        }
        response = httpx.get(url, headers=headers, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è§£æ Trending åˆ—è¡¨
        for article in soup.select('article.Box-row')[:20]:
            
            # é¡¹ç›®åç§°å’Œé“¾æ¥
            h2 = article.select_one('h2 a')
            if not h2:
                continue
            
            repo_path = h2.get('href', '').strip('/')
            repo_url = f"https://github.com/{repo_path}"
            repo_name = repo_path.split('/')[-1] if '/' in repo_path else repo_path
            
            # æè¿°
            desc_elem = article.select_one('p')
            description = desc_elem.get_text(strip=True) if desc_elem else ''
            
            # è¯­è¨€
            lang_elem = article.select_one('[itemprop="programmingLanguage"]')
            lang = lang_elem.get_text(strip=True) if lang_elem else 'Unknown'
            
            # ä»Šæ—¥ Star å¢é‡
            star_elem = article.select_one('.float-sm-right')
            today_stars = 0
            if star_elem:
                star_text = star_elem.get_text(strip=True)
                match = re.search(r'([\d,]+)', star_text)
                if match:
                    today_stars = int(match.group(1).replace(',', ''))
            
            # å…³é”®è¯åŒ¹é…
            keywords = ['graphics', 'rendering', 'ai', 'ml', 'neural', '3d', 'gpu', 'cuda']
            score = sum(1 for kw in keywords if kw in description.lower() or kw in repo_name.lower())
            
            items.append(NewsItem(
                title=repo_name,
                url=repo_url,
                source='GitHub',
                category=lang,
                summary=description[:150],
                score=today_stars + score * 10,
                extra={'today_stars': today_stars, 'language': lang}
            ))
            
    except Exception as e:
        print(f"[WARN] è·å– GitHub Trending å¤±è´¥: {e}")
    
    items.sort(key=lambda x: x.score, reverse=True)
    return items


# ============================================================================
#                        Hacker News æŠ“å–æ¨¡å—
# ============================================================================

def fetch_hackernews(keywords: list[str], min_score: int = 50) -> list[NewsItem]:
    """
    ä» Hacker News è·å–ç›¸å…³çƒ­å¸–
    
    Args:
        keywords: ç­›é€‰å…³é”®è¯
        min_score: æœ€ä½åˆ†æ•°é˜ˆå€¼
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    items = []
    
    print("[INFO] æ­£åœ¨è·å– Hacker News çƒ­å¸–...")
    
    try:
        # è·å– Top Stories
        top_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        headers = {'User-Agent': USER_AGENT}
        response = httpx.get(top_url, headers=headers, timeout=30)
        story_ids = response.json()[:100]  # å–å‰ 100
        
        for story_id in story_ids:
            item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            item_resp = httpx.get(item_url, headers=headers, timeout=10)
            story = item_resp.json()
            
            if not story or story.get('type') != 'story':
                continue
            
            title = story.get('title', '')
            score = story.get('score', 0)
            comments = story.get('descendants', 0)
            
            # åˆ†æ•°è¿‡æ»¤
            if score < min_score:
                continue
            
            # å…³é”®è¯åŒ¹é…
            title_lower = title.lower()
            matched = any(kw.lower() in title_lower for kw in keywords)
            
            if not matched:
                continue
            
            items.append(NewsItem(
                title=title,
                url=story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                source='HackerNews',
                category='Discussion',
                score=score,
                comments=comments,
                extra={'hn_id': story_id}
            ))
            
            # é™åˆ¶è¯·æ±‚é¢‘ç‡
            if len(items) >= 15:
                break
                
    except Exception as e:
        print(f"[WARN] è·å– Hacker News å¤±è´¥: {e}")
    
    return items


# ============================================================================
#                         Reddit æŠ“å–æ¨¡å—
# ============================================================================

def fetch_reddit(
    subreddits: list[str] = None,
    min_upvotes: int = 50,
    limit: int = 15
) -> list[NewsItem]:
    """
    ä» Reddit è·å–çƒ­é—¨å¸–å­
    
    Args:
        subreddits: ç›®æ ‡ Subreddit åˆ—è¡¨
        min_upvotes: æœ€ä½ upvotes é˜ˆå€¼
        limit: æ¯ä¸ª subreddit è·å–æ•°é‡
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    # ä»å…¨å±€é…ç½®åŠ è½½é»˜è®¤å€¼
    config = GLOBAL_CONFIG.get('sources', {}).get('reddit', {})
    
    if subreddits is None:
        subreddits = config.get('subreddits', [
            'MachineLearning', 'GraphicsProgramming', 'computergraphics',
            'LocalLLaMA', 'artificial', 'unrealengine', 'unrealengine5', 'gamedev'
        ])
    
    if min_upvotes == 50 and config.get('min_upvotes'):
         min_upvotes = config.get('min_upvotes')

    items = []
    # ä½¿ç”¨æ›´çœŸå®çš„ User-Agent
    headers = {
        'User-Agent': USER_AGENT
    }
    
    for sub in subreddits:
        print(f"[INFO] æ­£åœ¨è·å– r/{sub}...")
        url = f"https://www.reddit.com/r/{sub}/hot.json?limit={limit}"
        
        try:
            response = httpx.get(url, headers=headers, timeout=30)
            data = response.json()
            
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post.get('data', {})
                
                # è·³è¿‡ç½®é¡¶å’Œå¹¿å‘Š
                if post_data.get('stickied') or post_data.get('is_self') is None:
                    continue
                
                title = post_data.get('title', '')
                score = post_data.get('ups', 0)
                comments = post_data.get('num_comments', 0)
                permalink = post_data.get('permalink', '')
                
                # è¿‡æ»¤ä½çƒ­åº¦
                if score < min_upvotes:
                    continue
                
                # å°è¯•æå–å›¾ç‰‡
                image_url = ""
                preview = post_data.get('preview', {})
                images = preview.get('images', [])
                if images:
                    image_url = images[0].get('source', {}).get('url', '').replace('&amp;', '&')
                elif post_data.get('thumbnail') and post_data.get('thumbnail').startswith('http'):
                    image_url = post_data.get('thumbnail')

                items.append(NewsItem(
                    title=title,
                    url=f"https://www.reddit.com{permalink}",
                    source='Reddit',
                    category=f"r/{sub}",
                    score=score,
                    comments=comments,
                    image_url=image_url,
                    extra={
                        'subreddit': sub
                    }
                ))
                
        except Exception as e:
            print(f"[WARN] è·å– r/{sub} å¤±è´¥: {e}")
    
    # æ™ºèƒ½æ’åºï¼šç¡®ä¿æ¯ä¸ªç¤¾åŒºè‡³å°‘æœ‰ 1 æ¡ï¼ŒUE ç¤¾åŒºä¼˜å…ˆä¿è¯ 2 æ¡
    # 1. æŒ‰ç¤¾åŒºåˆ†ç»„
    by_subreddit = {}
    for item in items:
        sub = item.extra.get('subreddit', '')
        if sub not in by_subreddit:
            by_subreddit[sub] = []
        by_subreddit[sub].append(item)
    
    # 2. æ¯ä¸ªç¤¾åŒºæŒ‰çƒ­åº¦æ’åº
    for sub in by_subreddit:
        by_subreddit[sub].sort(key=lambda x: x.score, reverse=True)
    
    # 3. ä¼˜å…ˆé€‰å–ï¼šUE ç¤¾åŒºå„å– 2 æ¡ï¼Œå…¶ä»–ç¤¾åŒºå„å– 1 æ¡
    priority_subs = ['unrealengine', 'unrealengine5', 'gamedev']  # ä¼˜å…ˆç¤¾åŒº
    result = []
    
    # ä¼˜å…ˆç¤¾åŒºå„å– 2 æ¡
    for sub in priority_subs:
        if sub in by_subreddit:
            result.extend(by_subreddit[sub][:2])
            by_subreddit[sub] = by_subreddit[sub][2:]
    
    # å…¶ä»–ç¤¾åŒºå„å– 1 æ¡
    for sub, sub_items in by_subreddit.items():
        if sub not in priority_subs and sub_items:
            result.append(sub_items[0])
            by_subreddit[sub] = sub_items[1:]
    
    # 4. å‰©ä½™ä½ç½®æŒ‰çƒ­åº¦å¡«å……
    remaining = []
    for sub_items in by_subreddit.values():
        remaining.extend(sub_items)
    remaining.sort(key=lambda x: x.score, reverse=True)
    
    # å¡«å……åˆ° 20 æ¡
    slots_left = 20 - len(result)
    result.extend(remaining[:slots_left])
    
    # æœ€ç»ˆæŒ‰çƒ­åº¦æ’åºå±•ç¤º
    result.sort(key=lambda x: x.score, reverse=True)
    return result[:20]


# ============================================================================
#                     CG å›¾å½¢å­¦ä¸“å±æŠ“å–æ¨¡å—
# ============================================================================

def fetch_cg_graphics(
    min_upvotes: int = 10,
    limit: int = 15
) -> list[NewsItem]:
    """
    ä»å¤šä¸ª CG å›¾å½¢å­¦ç›¸å…³ç¤¾åŒºè·å–çƒ­é—¨å†…å®¹
    
    è¦†ç›–é¢†åŸŸ:
        - Unreal Engine (r/unrealengine, r/unrealengine5)
        - Three.js (r/threejs)
        - Blender (r/blender, r/blenderhelp)
        - Cinema 4D (r/Cinema4D)
        - Houdini (r/Houdini)
        - ShaderToy / Shaders (r/shaders, r/opengl)
        - é€šç”¨ CG (r/computergraphics, r/GraphicsProgramming)
    
    Args:
        min_upvotes: æœ€ä½ upvotes é˜ˆå€¼
        limit: æ¯ä¸ªç¤¾åŒºè·å–æ•°é‡
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨ï¼ŒAI ç›¸å…³å†…å®¹ä¼˜å…ˆ
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    # CG å›¾å½¢å­¦ä¸“å±ç¤¾åŒºåˆ—è¡¨
    cg_subreddits = [
        # Unreal Engine ç”Ÿæ€
        ('unrealengine', 'Unreal Engine'),
        ('unrealengine5', 'UE5'),
        # Three.js / WebGL
        ('threejs', 'Three.js'),
        # Blender ç”Ÿæ€
        ('blender', 'Blender'),
        ('blenderhelp', 'Blender Help'),
        # Cinema 4D
        ('Cinema4D', 'Cinema 4D'),
        # Houdini
        ('Houdini', 'Houdini'),
        # Shaders / ShaderToy
        ('shaders', 'Shaders'),
        ('opengl', 'OpenGL'),
        # é€šç”¨ CG
        ('computergraphics', 'CG'),
        ('GraphicsProgramming', 'Graphics'),
    ]
    
    # AI ç›¸å…³å…³é”®è¯ï¼ˆç”¨äºä¼˜å…ˆæ’åºï¼‰
    ai_keywords = [
        'ai', 'machine learning', 'ml', 'neural', 'deep learning',
        'gpt', 'llm', 'diffusion', 'stable diffusion', 'midjourney',
        'generative', 'procedural', 'automated', 'artificial intelligence',
        'comfyui', 'automatic', 'dall-e', 'dalle', 'sora', 'kling',
        'nerf', 'gaussian', 'splat', '3d gaussian', 'radiance field'
    ]
    
    items = []
    headers = {
        'User-Agent': USER_AGENT
    }
    
    for sub, label in cg_subreddits:
        print(f"[INFO] æ­£åœ¨è·å– r/{sub} ({label})...")
        url = f"https://www.reddit.com/r/{sub}/hot.json?limit={limit}"
        
        try:
            response = httpx.get(url, headers=headers, timeout=30)
            data = response.json()
            
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post.get('data', {})
                
                # è·³è¿‡ç½®é¡¶å’Œå¹¿å‘Š
                if post_data.get('stickied'):
                    continue
                
                title = post_data.get('title', '')
                score = post_data.get('ups', 0)
                comments = post_data.get('num_comments', 0)
                permalink = post_data.get('permalink', '')
                selftext = post_data.get('selftext', '')[:200]  # å–å‰200å­—ç¬¦
                
                # è¿‡æ»¤ä½çƒ­åº¦
                if score < min_upvotes:
                    continue
                
                # æ£€æµ‹æ˜¯å¦ AI ç›¸å…³
                text_lower = (title + ' ' + selftext).lower()
                is_ai_related = any(kw in text_lower for kw in ai_keywords)
                
                items.append(NewsItem(
                    title=title,
                    url=f"https://www.reddit.com{permalink}",
                    source='Reddit-CG',
                    category=label,
                    score=score,
                    comments=comments,
                    summary=selftext,
                    extra={
                        'subreddit': sub,
                        'label': label,
                        'is_ai_related': is_ai_related
                    }
                ))
                
        except Exception as e:
            print(f"[WARN] è·å– r/{sub} å¤±è´¥: {e}")
    
    # æ™ºèƒ½æ’åºï¼šAI ç›¸å…³ä¼˜å…ˆï¼Œç„¶åæŒ‰çƒ­åº¦
    # åˆ†ç¦» AI ç›¸å…³å’Œæ™®é€šå†…å®¹
    ai_items = [item for item in items if item.extra.get('is_ai_related')]
    normal_items = [item for item in items if not item.extra.get('is_ai_related')]
    
    # å„è‡ªæŒ‰çƒ­åº¦æ’åº
    ai_items.sort(key=lambda x: x.score, reverse=True)
    normal_items.sort(key=lambda x: x.score, reverse=True)
    
    # AI ç›¸å…³ä¼˜å…ˆï¼Œç„¶åæ™®é€šå†…å®¹
    result = ai_items + normal_items
    
    # ç¡®ä¿æ¯ä¸ªé¢†åŸŸè‡³å°‘æœ‰ 1 æ¡ï¼ˆå¤šæ ·æ€§ä¿è¯ï¼‰
    seen_labels = set()
    diverse_result = []
    remaining = []
    
    for item in result:
        label = item.extra.get('label')
        if label not in seen_labels:
            diverse_result.append(item)
            seen_labels.add(label)
        else:
            remaining.append(item)
    
    # å¡«å……å‰©ä½™ä½ç½®
    diverse_result.extend(remaining)
    
    # åˆå¹¶å®˜æ–¹æºï¼ˆæ›´æƒå¨ï¼‰
    official_items = fetch_cg_official()
    
    # å®˜æ–¹æºæ”¾åœ¨å‰é¢ï¼ˆä¼˜å…ˆå±•ç¤ºï¼‰ï¼Œç„¶åæ˜¯ç¤¾åŒºå†…å®¹
    # ä½†ä»ä¿æŒ AI ç›¸å…³ä¼˜å…ˆ
    all_items = official_items + diverse_result
    
    # é‡æ–°æŒ‰ AI ç›¸å…³æ€§å’Œåˆ†æ•°æ’åº
    ai_items = [item for item in all_items if item.extra.get('is_ai_related')]
    normal_items = [item for item in all_items if not item.extra.get('is_ai_related')]
    
    # å®˜æ–¹æºæ ‡è®° ğŸ›ï¸ï¼Œç¤¾åŒºæºä¿æŒåŸæ ·
    final_result = ai_items + normal_items
    
    return final_result[:30]  # CG ç‰ˆå—è¿”å›æ›´å¤šæ¡ç›®ï¼ˆå«å®˜æ–¹æºï¼‰


# ============================================================================
#                     CG å®˜æ–¹æºæŠ“å–æ¨¡å—
# ============================================================================

def fetch_cg_official() -> list[NewsItem]:
    """
    ä» CG è½¯ä»¶å®˜æ–¹åšå®¢/RSS è·å–æœ€æ–°èµ„è®¯
    
    å®˜æ–¹æºï¼ˆæ›´æƒå¨ã€æ›´å³æ—¶ï¼‰:
        - Unreal Engine å®˜æ–¹åšå®¢
        - Blender å®˜æ–¹åšå®¢
        - Three.js GitHub Releases
        - Houdini (SideFX) å®˜æ–¹åšå®¢
        - Maxon (Cinema 4D) å®˜æ–¹åšå®¢
        - Unity å®˜æ–¹åšå®¢
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    import feedparser
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    # å®˜æ–¹ RSS æºåˆ—è¡¨
    official_feeds = [
        # Unreal Engine
        {
            'name': 'Unreal Engine',
            'url': 'https://www.unrealengine.com/en-US/rss',
            'label': 'UE Official',
            'type': 'rss'
        },
        # Blender
        {
            'name': 'Blender',
            'url': 'https://www.blender.org/feed/',
            'label': 'Blender Official',
            'type': 'rss'
        },
        # Three.js GitHub Releases
        {
            'name': 'Three.js',
            'url': 'https://github.com/mrdoob/three.js/releases.atom',
            'label': 'Three.js Releases',
            'type': 'atom'
        },
        # SideFX Houdini
        {
            'name': 'Houdini',
            'url': 'https://www.sidefx.com/feed/',
            'label': 'Houdini Official',
            'type': 'rss'
        },
        # Unity
        {
            'name': 'Unity',
            'url': 'https://blog.unity.com/feed',
            'label': 'Unity Official',
            'type': 'rss'
        },
        # Godot Engine
        {
            'name': 'Godot',
            'url': 'https://godotengine.org/rss.xml',
            'label': 'Godot Official',
            'type': 'rss'
        },
        # NVIDIA Developer (Graphics)
        {
            'name': 'NVIDIA',
            'url': 'https://developer.nvidia.com/blog/feed/',
            'label': 'NVIDIA Dev',
            'type': 'rss'
        },
    ]
    
    # AI ç›¸å…³å…³é”®è¯
    ai_keywords = [
        'ai', 'machine learning', 'neural', 'deep learning',
        'diffusion', 'generative', 'gaussian', 'nerf', 'dlss',
        'ray tracing', 'rtx', 'tensor'
    ]
    
    items = []
    
    for feed_info in official_feeds:
        name = feed_info['name']
        url = feed_info['url']
        label = feed_info['label']
        
        print(f"[INFO] æ­£åœ¨è·å– {name} å®˜æ–¹æº...")
        
        try:
            feed = feedparser.parse(url)
            
            if not feed.entries:
                print(f"[WARN] {name} æ— æ³•è·å–æˆ–æ— å†…å®¹")
                continue
            
            for entry in feed.entries[:5]:  # æ¯ä¸ªæºå–å‰5æ¡
                title = entry.get('title', '')
                link = entry.get('link', '')
                summary = sanitize_html_text(entry.get('summary', entry.get('description', '')), max_length=200)
                published = entry.get('published', entry.get('updated', ''))
                
                # æ¸…ç† HTML æ ‡ç­¾
                title = re.sub(r'<[^>]+>', '', title).strip()
                summary = re.sub(r'<[^>]+>', '', summary).strip()
                
                # æ£€æµ‹æ˜¯å¦ AI ç›¸å…³
                text_lower = (title + ' ' + summary).lower()
                is_ai_related = any(kw in text_lower for kw in ai_keywords)
                
                items.append(NewsItem(
                    title=title,
                    url=link,
                    source='Official',
                    category=label,
                    score=100 if is_ai_related else 50,  # AI ç›¸å…³ç»™é«˜åˆ†
                    summary=summary,
                    date=published,
                    extra={
                        'software': name,
                        'label': label,
                        'is_ai_related': is_ai_related,
                        'is_official': True
                    }
                ))
                
        except Exception as e:
            print(f"[WARN] è·å– {name} å®˜æ–¹æºå¤±è´¥: {e}")
    
    # AI ç›¸å…³ä¼˜å…ˆï¼Œç„¶åæŒ‰æ—¥æœŸ/åˆ†æ•°æ’åº
    ai_items = [item for item in items if item.extra.get('is_ai_related')]
    normal_items = [item for item in items if not item.extra.get('is_ai_related')]
    
    ai_items.sort(key=lambda x: x.score, reverse=True)
    normal_items.sort(key=lambda x: x.score, reverse=True)
    
    result = ai_items + normal_items
    
    # ç¡®ä¿æ¯ä¸ªè½¯ä»¶è‡³å°‘æœ‰ 1 æ¡
    seen_software = set()
    diverse_result = []
    remaining = []
    
    for item in result:
        software = item.extra.get('software')
        if software not in seen_software:
            diverse_result.append(item)
            seen_software.add(software)
        else:
            remaining.append(item)
    
    diverse_result.extend(remaining)
    
    return diverse_result[:15]


# ============================================================================
#                        Twitter/X æŠ“å–æ¨¡å— (via Nitter)
# ============================================================================

def fetch_bluesky(
    accounts: list[str] = None,
    keywords: list[str] = None,
    limit: int = 15
) -> list[NewsItem]:
    """
    ä» Bluesky è·å– KOL åŠ¨æ€ï¼ˆä½¿ç”¨ AT Protocol å…¬å¼€ APIï¼‰
    
    Args:
        accounts: Bluesky è´¦å·åˆ—è¡¨ï¼ˆæ ¼å¼ï¼šhandle.bsky.socialï¼‰
        keywords: AI/CG/UE å…³é”®è¯è¿‡æ»¤
        limit: æœ€å¤§è·å–æ•°é‡
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    if accounts is None:
        # AI/CG/UE é¢†åŸŸæ´»è·ƒè´¦å·
        accounts = [
            'jay.bsky.team',           # Bluesky å®˜æ–¹
            'simonwillison.net',       # AI å¼€å‘è€…
            'stratechery.com',         # ç§‘æŠ€åˆ†æ
            'arstechnica.com',         # ç§‘æŠ€æ–°é—»
        ]
    
    if keywords is None:
        keywords = [
            'ai', 'gpt', 'llm', 'diffusion', 'neural', 'rendering',
            '3d', 'graphics', 'cuda', 'gpu', 'transformer', 'model',
            'paper', 'research', 'release', 'open source',
            'unreal', 'ue5', 'game dev', 'nanite', 'lumen'  # UE ç›¸å…³
        ]
    
    items = []
    headers = {
        'User-Agent': USER_AGENT
    }
    
    for account in accounts:
        print(f"[INFO] æ­£åœ¨è·å– @{account} çš„ Bluesky å¸–å­...")
        
        # è§£æ DID
        try:
            resolve_url = f"https://bsky.social/xrpc/com.atproto.identity.resolveHandle?handle={account}"
            resolve_resp = httpx.get(resolve_url, headers=headers, timeout=10)
            
            if resolve_resp.status_code != 200:
                print(f"[WARN] æ— æ³•è§£æ @{account}")
                continue
            
            did = resolve_resp.json().get('did', '')
            if not did:
                continue
            
            # è·å–å¸–å­
            feed_url = f"https://bsky.social/xrpc/app.bsky.feed.getAuthorFeed?actor={did}&limit=20"
            feed_resp = httpx.get(feed_url, headers=headers, timeout=15)
            
            if feed_resp.status_code != 200:
                print(f"[WARN] æ— æ³•è·å– @{account} çš„å¸–å­")
                continue
            
            feed_data = feed_resp.json()
            posts = feed_data.get('feed', [])
            
            for post_item in posts[:10]:
                post = post_item.get('post', {})
                record = post.get('record', {})
                
                text = record.get('text', '')[:200]
                created_at = record.get('createdAt', '')
                uri = post.get('uri', '')
                
                # æ„å»º Bluesky ç½‘é¡µé“¾æ¥
                author = post.get('author', {})
                handle = author.get('handle', '')
                rkey = uri.split('/')[-1] if uri else ''
                web_url = f"https://bsky.app/profile/{handle}/post/{rkey}" if handle and rkey else ''
                
                # å…³é”®è¯åŒ¹é…
                text_lower = text.lower()
                matched = any(kw.lower() in text_lower for kw in keywords)
                
                if matched and text:
                    items.append(NewsItem(
                        title=text,
                        url=web_url,
                        source='Bluesky',
                        category=f"@{account}",
                        date=created_at,
                        extra={
                            'account': account,
                            'handle': handle
                        }
                    ))
                    
        except Exception as e:
            print(f"[WARN] è·å– @{account} å¤±è´¥: {e}")
    
    return items[:limit]


# ä¿ç•™ fetch_twitter ä½œä¸ºåˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
def fetch_twitter(*args, **kwargs):
    """å·²å¼ƒç”¨ï¼šTwitter API ä¸å†å¯ç”¨ï¼Œè¯·ä½¿ç”¨ fetch_bluesky"""
    print("[WARN] Twitter æ•°æ®æºå·²å¼ƒç”¨ï¼ˆAPI å…³é—­ï¼‰ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° Bluesky")
    return fetch_bluesky(*args, **kwargs)


# ============================================================================
#                        Product Hunt æŠ“å–æ¨¡å—
# ============================================================================

def fetch_product_hunt(limit: int = 15) -> list[NewsItem]:
    """
    ä» Product Hunt è·å–çƒ­é—¨äº§å“ (RSS)
    """
    try:
        import feedparser
    except ImportError:
        print("[ERROR] è¯·å®‰è£… feedparser: pip install feedparser")
        return []

    config = GLOBAL_CONFIG.get('sources', {}).get('product_hunt', {})
    if not config.get('enabled', True):
        return []

    url = config.get('rss_url', 'https://www.producthunt.com/feed')
    min_votes = config.get('min_votes', 0)

    print(f"[INFO] æ­£åœ¨è·å– Product Hunt...")
    
    items = []
    try:
        # æ·»åŠ  User-Agenté˜²æ­¢è¢«æ‹¦æˆª
        feed = feedparser.parse(url, agent=USER_AGENT)
        for entry in feed.entries[:limit]:
            title = entry.title
            link = entry.link
            content = entry.get('summary', '') or entry.get('content', [{}])[0].get('value', '')
            
            # æå–æŠ•ç¥¨æ•° - å°è¯•å¤šä¸ªå¯èƒ½çš„æ¥æº
            score = 0
            # æ–¹æ³•1: ä» content ä¸­æå– Votes: X
            votes_match = re.search(r'Votes:\s*(\d+)', content)
            if votes_match:
                score = int(votes_match.group(1))
            # æ–¹æ³•2: ä» entry çš„ tags ä¸­æŸ¥æ‰¾æŠ•ç¥¨æ•°ï¼ˆæŸäº› RSS ç‰ˆæœ¬ï¼‰
            if score == 0 and hasattr(entry, 'tags'):
                for tag in entry.tags:
                    if hasattr(tag, 'term'):
                        vote_match = re.search(r'(\d+)\s*votes?', tag.term, re.IGNORECASE)
                        if vote_match:
                            score = int(vote_match.group(1))
                            break
            # æ–¹æ³•3: å°è¯•ä» summary ä¸­æå–æ•°å­—ï¼ˆå¦‚ "123 votes"ï¼‰
            if score == 0:
                vote_match = re.search(r'(\d+)\s*(?:votes?|upvotes?)', content, re.IGNORECASE)
                if vote_match:
                    score = int(vote_match.group(1))
            
            if score < min_votes:
                continue

            # æå–å›¾ç‰‡
            image_url = ""
            img_match = re.search(r'img src="([^"]+)"', content)
            if img_match:
                image_url = img_match.group(1)
            
            # æ¸…ç†æ‘˜è¦ HTML
            summary = re.sub(r'<[^>]+>', ' ', content).strip()
            # ç§»é™¤ "Comments: X, Votes: Y" åŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹
            summary = re.split(r'Comments:\s*\d+\s*,\s*Votes:\s*\d+', summary, flags=re.IGNORECASE)[0].strip()
            # ç§»é™¤ "Discussion | Link" ç­‰å¤šä½™æ–‡æœ¬ï¼ˆæ”¯æŒå¤šç§å˜ä½“ï¼‰
            summary = re.sub(r'\s*Discussion\s*\|\s*Link.*$', '', summary, flags=re.IGNORECASE).strip()
            summary = re.sub(r'\s*Discussion\s*$', '', summary, flags=re.IGNORECASE).strip()
            summary = re.sub(r'\|\s*Link.*$', '', summary, flags=re.IGNORECASE).strip()
            # æ¸…ç†å¤šä½™ç©ºç™½ï¼ˆåŒ…æ‹¬æ¢è¡Œç¬¦ï¼‰
            summary = ' '.join(summary.split())
            # æœ€åå†æ¬¡æ£€æŸ¥å¹¶ç§»é™¤ Discussion/Link æ®‹ç•™
            if 'Discussion' in summary or '|' in summary:
                parts = re.split(r'(?:Discussion|\|)', summary)
                if parts:
                    summary = parts[0].strip()
            
            items.append(NewsItem(
                title=title,
                url=link,
                source='ProductHunt',
                category='Product',
                score=score,
                summary=summary[:200],
                image_url=image_url
            ))
            
    except Exception as e:
        print(f"[WARN] è·å– Product Hunt å¤±è´¥: {e}")
        
    return items

# ============================================================================
#                     Trending Skills æŠ“å–æ¨¡å— (skills.sh)
# ============================================================================

def fetch_trending_skills(limit: int = 15) -> list[NewsItem]:
    """
    ä» skills.sh è·å–çƒ­é—¨ AI Agent æŠ€èƒ½
    """
    try:
        import httpx
        from bs4 import BeautifulSoup
    except ImportError:
        print("[ERROR] è¯·å®‰è£…ä¾èµ–: pip install httpx beautifulsoup4")
        return []
    
    # æ£€æŸ¥é…ç½®æ˜¯å¦å¯ç”¨
    config = GLOBAL_CONFIG.get('sources', {}).get('trending_skills', {})
    if not config.get('enabled', False):
         return []
    
    limit = config.get('limit', limit)
    
    print("[INFO] æ­£åœ¨è·å– Trending Skills (skills.sh)...")
    url = "https://skills.sh/trending"
    items = []
    
    try:
        headers = {
            'User-Agent': USER_AGENT
        }
        response = httpx.get(url, headers=headers, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç®€å•æ–‡æœ¬è¡Œåˆ†æ (å‚è€ƒåŸ repo é€»è¾‘)
        text = soup.get_text("\n", strip=True)
        lines = text.split('\n')
        
        start_parsing = False
        parsed_count = 0
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # å®šä½æ¦œå•å¼€å§‹
            if "Leaderboard" in line or "Skills Leaderboard" in line:
                start_parsing = True
                i += 1
                continue
            
            if not start_parsing:
                i += 1
                continue
                
            # å°è¯•åŒ¹é… Rank (æ•°å­—)
            if line.isdigit():
                try:
                    rank = int(line)
                    if i + 1 < len(lines):
                        name = lines[i+1]
                        # ç®€å•çš„éªŒè¯: name åº”è¯¥æ˜¯ kebab-case
                        if not re.match(r'^[a-z0-9-]+$', name):
                             i += 1
                             continue

                        # Owner/Repo (ä¸‹ä¸€è¡Œ)
                        owner_repo = ""
                        if i + 2 < len(lines):
                            owner_repo = lines[i+2]
                        
                        # æ„é€  item
                        # URL æ ¼å¼: https://skills.sh/{owner}/{repo}/{skill_name}
                        url = f"https://skills.sh/{owner_repo}/{name}"
                        
                        items.append(NewsItem(
                            title=name,
                            url=url,
                            source='Skills.sh',
                            category='Agent Skill',
                            score=100 - parsed_count, # æ¨¡æ‹Ÿåˆ†æ•°
                            summary=f"Rank #{rank} on skills.sh. Owner: {owner_repo}",
                            tags=['AI Agent', 'Skill'], # Default tags
                            extra={'rank': rank, 'owner': owner_repo}
                        ))
                        parsed_count += 1
                        
                        if parsed_count >= limit:
                            break
                        
                        # è·³è¿‡å·²å¤„ç†çš„è¡Œ
                        i += 3
                        continue
                except:
                    pass
            
            i += 1
            
    except Exception as e:
        print(f"[WARN] è·å– Trending Skills å¤±è´¥: {e}")
        
    items.sort(key=lambda x: x.extra.get('rank', 999))
    return items

def fetch_huggingface_papers(limit: int = 10) -> list[NewsItem]:
    """
    ä» Hugging Face Daily Papers è·å–çƒ­é—¨è®ºæ–‡
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []

    config = GLOBAL_CONFIG.get('sources', {}).get('huggingface', {})
    if not config.get('enabled', True):
        return []
    
    limit = config.get('limit', limit)
    url = "https://huggingface.co/api/daily_papers"
    print(f"[INFO] æ­£åœ¨è·å– Hugging Face Daily Papers...")
    
    items = []
    try:
        headers = {
            'User-Agent': USER_AGENT
        }
        response = httpx.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # data æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'paper' é”®
        for entry in data[:limit]:
            paper = entry.get('paper', {})
            title = paper.get('title', 'Untitled')
            paper_id = paper.get('id', '')
            link = f"https://huggingface.co/papers/{paper_id}" if paper_id else "https://huggingface.co/papers"
            
            summary = paper.get('summary', '')
            # ä¼˜å…ˆä½¿ç”¨ ai_summary å¦‚æœæœ‰
            if paper.get('ai_summary'):
                summary = paper.get('ai_summary')

            upvotes = paper.get('upvotes', 0)
            thumbnail = paper.get('thumbnail', '')
            
            # ä½œè€…å¤„ç†
            authors = paper.get('authors', [])
            author_names = [a.get('name', 'Unknown') for a in authors[:3]]
            author_str = ", ".join(author_names)
            
            items.append(NewsItem(
                title=title,
                url=link,
                source='HuggingFace',
                category='Paper',
                summary=summary,
                score=upvotes,
                date=paper.get('publishedAt', ''),
                extra={
                    'thumbnail': thumbnail,
                    'upvotes': upvotes,
                    'is_hf': True
                }
            ))
            
    except Exception as e:
        print(f"[WARN] è·å– Hugging Face Papers å¤±è´¥: {e}")
        
    items.sort(key=lambda x: x.score, reverse=True)
    return items


# ============================================================================
#                        æ™ºèƒ½è¿‡æ»¤æ¨¡å—
# ============================================================================

def is_blacklisted(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯"""
    if not text:
        return False
    
    blacklist = GLOBAL_CONFIG.get('filtering', {}).get('exclude_keywords', [])
    if not blacklist:
        # é»˜è®¤é»‘åå•
        blacklist = ["blockchain", "crypto", "nft", "web3", "bitcoin", "ethereum", "token"]
        
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in blacklist)

def deduplicate_items(items: list[NewsItem]) -> list[NewsItem]:
    """å»é‡ï¼šä¿ç•™åˆ†æ•°æœ€é«˜çš„ URL"""
    if not GLOBAL_CONFIG.get('filtering', {}).get('deduplicate', True):
        return items
        
    seen_urls = {}
    unique_items = []
    
    for item in items:
        # å½’ä¸€åŒ– URL (ç§»é™¤æœ«å°¾æ–œæ ï¼Œç§»é™¤ utm å‚æ•°ç­‰ç®€å•å¤„ç†)
        url = item.url.split('?')[0].rstrip('/')
        
        if url in seen_urls:
            existing_item = seen_urls[url]
            # å¦‚æœå½“å‰é¡¹åˆ†æ•°æ›´é«˜ï¼Œæ›¿æ¢ï¼ˆä½†è¿™é‡Œé€»è¾‘ç¨å¾®å¤æ‚ï¼Œå› ä¸º items åˆ—è¡¨é¡ºåºé—®é¢˜ï¼‰
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä¼˜å…ˆä¿ç•™å…ˆå‡ºç°çš„ï¼ˆé€šå¸¸å„æºå†…éƒ¨å·²æŒ‰çƒ­åº¦æ’åºï¼‰ï¼Œæˆ–è€…åˆå¹¶ä¿¡æ¯
            # è¿™é‡Œé€‰æ‹©ä¿ç•™ç¬¬ä¸€ä¸ª
            continue
        
        seen_urls[url] = item
        unique_items.append(item)
        
    return unique_items


# ============================================================================
#                         ä¸­æ–‡æ¦‚è¿°ç”Ÿæˆæ¨¡å—
# ============================================================================

def generate_chinese_summary(text: str, max_length: int = 80, retries: int = 5) -> str:
    """
    ä¸ºè‹±æ–‡å†…å®¹ç”Ÿæˆä¸­æ–‡æ¦‚è¿°
    
    é‡‡ç”¨åŒé‡ç­–ç•¥:
    1. ç®€å•ç¿»è¯‘ï¼ˆä½¿ç”¨ Google Translate éå®˜æ–¹ APIï¼Œå¸¦é‡è¯•ï¼‰
    2. è‹¥å¤±è´¥åˆ™è¿”å›æˆªæ–­çš„åŸæ–‡å¹¶æ ‡è®° [EN]
    
    Args:
        text: è‹±æ–‡åŸæ–‡
        max_length: æœ€å¤§é•¿åº¦
        retries: é‡è¯•æ¬¡æ•°
    
    Returns:
        ä¸­æ–‡æ¦‚è¿°
    """
    if not text:
        return ""
    
    # æ¸…ç†æ–‡æœ¬
    text = re.sub(r'https?://\S+', '', text)  # ç§»é™¤é“¾æ¥
    text = re.sub(r'\s+', ' ', text).strip()  # åˆå¹¶ç©ºç™½
    original_text = text[:500]  # ä¿ç•™åŸæ–‡ç”¨äº fallback
    
    import time
    import random
    
    # æ¯æ¬¡è°ƒç”¨å‰éšæœºå»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
    time.sleep(random.uniform(0.3, 0.8))
    
    for attempt in range(retries):
        try:
            import httpx
            
            # Google Translate éå®˜æ–¹ API
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'auto',  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                'tl': 'zh-CN',
                'dt': 't',
                'q': original_text
            }
            
            response = httpx.get(url, params=params, timeout=20)
            result = response.json()
            
            # æå–ç¿»è¯‘ç»“æœ
            if result and result[0]:
                translated = ''.join([part[0] for part in result[0] if part[0]])
                # æˆªæ–­åˆ°åˆé€‚é•¿åº¦
                if len(translated) > max_length:
                    translated = translated[:max_length-3] + '...'
                return translated
                
        except Exception as e:
            if attempt < retries - 1:
                wait_time = 1.0 * (attempt + 1) + random.uniform(0.5, 1.5)
                time.sleep(wait_time)  # é€’å¢ç­‰å¾… + éšæœºæŠ–åŠ¨
            else:
                print(f"[WARN] ç¿»è¯‘å¤±è´¥ (é‡è¯• {retries} æ¬¡å): {e}")
    
    # é™çº§ï¼šè¿”å›æˆªæ–­çš„åŸæ–‡ï¼Œæ ‡è®° [EN]
    fallback = original_text[:max_length-8] + '...' if len(original_text) > max_length-5 else original_text
    return f"[EN] {fallback}"


# ============================================================================
#                           æŠ¥å‘Šç”Ÿæˆæ¨¡å—
# ============================================================================

def _generate_html_card(item: NewsItem, summary: str, meta_left: str, meta_right: str) -> str:
    """ç”Ÿæˆ Quora é£æ ¼çš„æ–°é—»å¡ç‰‡ HTML"""
    
    # é»˜è®¤å›¾ç‰‡ï¼ˆå¦‚æœæ˜¯ GitHubï¼Œä½¿ç”¨ OpenGraphï¼‰
    image_html = ""
    if item.image_url:
        image_html = f'<div class="news-card-image" style="background-image: url(\'{item.image_url}\')"></div>'
    elif item.source == 'GitHub':
        # GitHub OpenGraph Image æ„é€ 
        try:
            repo_path = item.url.replace('https://github.com/', '')
            og_url = f"https://opengraph.githubassets.com/1/{repo_path}"
            image_html = f'<div class="news-card-image" style="background-image: url(\'{og_url}\')"></div>'
        except:
            pass
    
    # å¸ƒå±€å†³å®šï¼šå¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨å¸¦å›¾ç‰‡çš„å¸ƒå±€ï¼›å¦åˆ™ä½¿ç”¨çº¯æ–‡æœ¬å¸ƒå±€
    has_image_class = " has-image" if image_html else ""
    
    # ç”Ÿæˆæ ‡ç­¾ï¼ˆå¦‚æœ item æ²¡æœ‰æ ‡ç­¾ï¼Œåˆ™ç°åœºç”Ÿæˆï¼‰
    tags = item.tags if item.tags else generate_tags(item.title, item.summary, item.source, item.category)
    # FIX: å¿…é¡»å°†ç”Ÿæˆçš„æ ‡ç­¾å›å†™åˆ° item å¯¹è±¡ï¼Œå¦åˆ™ä¸‹é¢çš„ json dump å°†ä¸ºç©º
    if not item.tags and tags:
        item.tags = tags
    # tags_json = json.dumps(tags, ensure_ascii=False) # This line is removed as per the change
    
    # æ ‡ç­¾ HTMLï¼ˆå°å¾½ç« å½¢å¼ï¼‰
    tags_html = ""
    if tags:
        tag_badges = " ".join([f'<span class="news-tag">{tag}</span>' for tag in tags[:3]])  # æœ€å¤šæ˜¾ç¤º3ä¸ª
        tags_html = f'\n        <div class="news-tags">{tag_badges}</div>'
    
    # ç”Ÿæˆ HTML
    # æ³¨æ„ï¼šæˆ‘ä»¬å°†æ ‡ç­¾æ•°æ®æ”¾åœ¨ä¸€ä¸ªéšè—çš„ div ä¸­ï¼Œè€Œä¸æ˜¯çˆ¶ div çš„ data-tags å±æ€§
    # å› ä¸ºæŸäº› Markdown è§£æå™¨ï¼ˆå¦‚ marked.jsï¼‰å¯èƒ½ä¼šåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å‰¥ç¦» data- å±æ€§
    tags_json = json.dumps(item.tags, ensure_ascii=False)
    
    # ç”Ÿæˆå›¾ç‰‡ HTML
    # Fix: Extract logic to avoid backslashes in f-string expressions (Python <3.12 syntax error)
    image_div = ""
    card_class = "news-card"
    if item.image_url:
        card_class += " has-image"
        image_div = f'<div class="news-card-image" style="background-image: url(\'{item.image_url}\');"></div>'

    # Fix: ç¡®ä¿ HTML ç»“æ„æ­£ç¡®ï¼Œimage_div å’Œ </div> åˆ†å¼€
    image_html = f"\n    {image_div}" if image_div else ""
    
    safe_summary = sanitize_html_text(summary or item.summary)

    # CRITICAL: ä¸èƒ½æœ‰ç¼©è¿›ï¼Markdown ä¼šæŠŠ 4 ç©ºæ ¼ç¼©è¿›å½“ä½œä»£ç å—
    html = f"""<div class="{card_class}">
<div class="news-tags-data" style="display:none">{tags_json}</div>
<div class="news-card-content">
<div class="news-card-header">
<span class="news-source-tag">{item.source}</span>
<span class="news-date">{item.date}</span>
</div>
<a href="{item.url}" target="_blank" class="news-title-link">
<h3 class="news-title">{item.title}</h3>
</a>
<div class="news-summary">{safe_summary}</div>{tags_html}
<div class="news-meta">
<span class="meta-left">{meta_left}</span>
<span class="meta-right">{meta_right}</span>
</div>
</div>{image_html}
</div>
"""
    return html

def generate_report(
    arxiv_items: list[NewsItem],
    github_items: list[NewsItem],
    hn_items: list[NewsItem],
    output_dir: str,
    reddit_items: list[NewsItem] = None,
    twitter_items: list[NewsItem] = None,
    cg_items: list[NewsItem] = None,
    ph_items: list[NewsItem] = None,
    trending_skills_items: list[NewsItem] = None,
    hf_items: list[NewsItem] = None, # Added
    with_summary: bool = False,
    report_date: str = None
) -> str:
    """
    ç”Ÿæˆ Markdown æ ¼å¼çš„æ–°é—»æŠ¥å‘Š (åµŒå…¥ HTML å¡ç‰‡)
    """
    if report_date:
        today = datetime.datetime.strptime(report_date, '%Y-%m-%d').date()
    else:
        today = datetime.date.today()
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    reddit_items = reddit_items or []
    twitter_items = twitter_items or []
    cg_items = cg_items or []
    hf_items = hf_items or []
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    filename = output_path / f"{today.isoformat()}.md"
    
    # æ„å»ºæŠ¥å‘Šå†…å®¹
    lines = [
        f"# ğŸ“° AI & CG æ¯æ—¥èµ„è®¯ - {today.isoformat()}",
        "",
        f"> è‡ªåŠ¨ç”Ÿæˆäº {timestamp}",
        "",
    ]
    
    # GitHub éƒ¨åˆ†
    if github_items:
        lines.extend([
            "## ğŸ”¥ GitHub Trending",
            "![GitHub Trending](../img/github.png)",
            '<div class="news-grid">',
        ])
        for item in github_items[:10]:
            if with_summary:
                desc = generate_chinese_summary(item.summary, 60)
            else:
                desc = item.summary[:60] + '...'
            
            today_stars = item.extra.get('today_stars', 0)
            lang = item.extra.get('language', 'Unknown')
            
            card = _generate_html_card(
                item, 
                desc, 
                f"ğŸ”¤ {lang}", 
                f"â­ +{today_stars}"
            )
            lines.append(card)
            
        lines.append('</div>') # End grid
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser
    
    # CG å›¾å½¢å­¦ä¸“å±ç‰ˆå—
    
    # Trending Skills éƒ¨åˆ†
    if trending_skills_items:
        lines.extend([
            "## ğŸ› ï¸ Trending Skills for Agents",
            "![Trending Skills](../img/skills.png)",
            "> Top Agent Skills from skills.sh",
            '<div class="news-grid">',
        ])
        for item in trending_skills_items[:10]:
            if with_summary:
                # æŠ€èƒ½æè¿°é€šå¸¸å¾ˆçŸ­ï¼Œæˆ–è€…ä¸å¦‚ title é‡è¦
                summary = item.summary
            else:
                 summary = item.summary
            
            card = _generate_html_card(
                item,
                summary,
                "ğŸ¤– Skill",
                f"#{item.extra.get('rank', 0)}"
            )
            lines.append(card)
        lines.append('</div>')
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser

    # Hugging Face éƒ¨åˆ†
    if hf_items:
        lines.extend([
            "## ğŸ¤— Hugging Face Papers",
            "![Hugging Face](../img/Hugging%20Face.png)",
            "> Daily Top Papers from hf.co/papers",
            '<div class="news-grid">',
        ])
        for item in hf_items[:10]:
            if with_summary:
                summary = generate_chinese_summary(item.summary, 80)
            else:
                summary = item.summary[:100] + '...'
            
            thumb = item.extra.get('thumbnail', '')
            upvotes = item.extra.get('upvotes', 0)
            
            # å¦‚æœæœ‰ç¼©ç•¥å›¾ï¼Œå¯ä»¥è€ƒè™‘åœ¨å¡ç‰‡ä¸­æ˜¾ç¤ºï¼Œè¿™é‡Œæš‚æ—¶ç”¨æ ‡å‡†å¡ç‰‡
            # å¯ä»¥åœ¨ meta ä¸­åŠ å›¾ç‰‡æ ‡è®°
            meta_left = "ğŸ“„ Paper"
            
            card = _generate_html_card(
                item,
                summary,
                meta_left,
                f"ğŸ‘ {upvotes}"
            )
            lines.append(card)
        lines.append('</div>')
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser

    # Product Hunt éƒ¨åˆ†
    if ph_items:
        lines.extend([
            "## ğŸš€ Product Hunt æ¯æ—¥ç²¾é€‰",
            "![Product Hunt](../img/product%20hunt.png)",
            '<div class="news-grid">',
        ])
        for item in ph_items[:10]:
            if with_summary:
                summary = generate_chinese_summary(item.summary, 80)
            else:
                summary = item.summary[:80] + '...'
            
            card = _generate_html_card(
                item,
                summary,
                "ğŸ†• Product",
                f"â–² {item.score}"
            )
            lines.append(card)
        lines.append('</div>')
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser

    if cg_items:
        lines.extend([
            "## ğŸ¨ CG å›¾å½¢å­¦",
            "![CG å›¾å½¢å­¦](../img/CG.png)",
            "> è¦†ç›–: Unreal Engine | Three.js | Blender | Houdini | Unity | Godot | NVIDIA",
            "",
            '<div class="news-grid">',
        ])
        for item in cg_items[:20]:
            if with_summary:
                summary_text = generate_chinese_summary(item.title, 80)
            else:
                summary_text = item.summary[:80] + '...'
            
            # æ ‡è®°
            marks = []
            if item.extra.get('is_official'): marks.append("ğŸ›ï¸ å®˜æ–¹")
            if item.extra.get('is_ai_related'): marks.append("ğŸ¤– AI")
            meta_left = " ".join(marks) if marks else "ğŸ”¥ çƒ­é—¨"
            
            card = _generate_html_card(
                item,
                summary_text,
                meta_left,
                f"ğŸ”¥ {item.score}"
            )
            lines.append(card)
            
        lines.append('</div>')
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser
    
    # Bluesky éƒ¨åˆ†
    if twitter_items:
        lines.extend([
            "## ğŸ¦‹ Bluesky åŠ¨æ€",
            '<div class="news-grid">',
        ])
        for item in twitter_items[:10]:
            if with_summary:
                summary = generate_chinese_summary(item.title, 80)
            else:
                summary = item.title[:80] + '...'
            
            card = _generate_html_card(
                item,
                summary,
                "ğŸ‘¤ KOL",
                "[åŸå¸–]"
            )
            lines.append(card)
        lines.append('</div>')
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser
    
    # Reddit éƒ¨åˆ†
    if reddit_items:
        lines.extend([
            "## ğŸ”´ Reddit è®¨è®º",
            "![Reddit è®¨è®º](../img/reddit.png)",
            '<div class="news-grid">',
        ])
        for item in reddit_items[:10]:
            if with_summary:
                summary = generate_chinese_summary(item.title, 80)
            else:
                summary = item.title[:80] + '...'
            
            card = _generate_html_card(
                item,
                summary,
                f"r/{item.extra.get('subreddit')}",
                f"ğŸ”¥ {item.score}"
            )
            lines.append(card)
        lines.append('</div>')
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser
    
    # Hacker News éƒ¨åˆ†
    if hn_items:
        lines.extend([
            "## ğŸ’¬ Hacker News çƒ­è®®",
            "![Hacker News](../img/Hacker%20News.png)",
            '<div class="news-grid">',
        ])
        for item in hn_items[:10]:
            if with_summary:
                summary = generate_chinese_summary(item.title, 80)
            else:
                summary = item.title[:80] + '...'
            
            card = _generate_html_card(
                item,
                summary,
                f"ğŸ’¬ {item.comments} è¯„è®º",
                f"Points: {item.score}"
            )
            lines.append(card)
        lines.append('</div>')
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser
    
    # arXiv å­¦æœ¯å‰æ²¿
    if arxiv_items:
        lines.extend([
            "## ğŸ“ å­¦æœ¯å‰æ²¿ (arXiv)",
            "![å­¦æœ¯å‰æ²¿](../img/arXiv.png)",
            '<div class="news-grid">',
        ])
        for item in arxiv_items[:10]: # é™åˆ¶æ•°é‡
            if with_summary:
                summary = generate_chinese_summary(item.title, 100)
            else:
                summary = item.title[:100] + '...'
            
            authors = item.authors[:30] + '...' if len(item.authors) > 30 else item.authors
            
            card = _generate_html_card(
                item,
                summary,
                f"âœï¸ {authors}",
                "ğŸ“„ PDF"
            )
            lines.append(card)
        lines.append('</div>')
        lines.append("")
        lines.append("")  # Extra blank line for Markdown parser
    
    # é¡µè„šï¼ˆç®€åŒ–ï¼šä»…åˆ†éš”çº¿ï¼Œç§»é™¤è‡ªåŠ¨ç”Ÿæˆæç¤ºï¼‰
    lines.append("---")
    
    # å†™å…¥æ–‡ä»¶
    content = '\n'.join(lines)
    filename.write_text(content, encoding='utf-8')
    
    print(f"[OK] æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    return str(filename)


# ============================================================================
#                              ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    # åŠ è½½é…ç½®
    load_config()

    parser = argparse.ArgumentParser(
        description='AI & CG æ–°é—»èšåˆè„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python fetch_news.py --all                    # è·å–æ‰€æœ‰æ¥æº
"""
    )
    
    parser.add_argument('--all', action='store_true', help='è·å–æ‰€æœ‰æ¥æº')
    parser.add_argument('--source', 
        choices=['arxiv', 'github', 'hn', 'reddit', 'bluesky', 'cg', 'ph', 'trending_skills', 'huggingface'],
        help='æŒ‡å®šåªè·å–æŸä¸ªæº')
    parser.add_argument('--categories', default=None, help='arXiv åˆ†ç±»ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--days', type=int, default=1, help='è·å–æœ€è¿‘å‡ å¤©çš„å†…å®¹')
    parser.add_argument('--output', default=None, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--keywords', default=None, help='HN/Twitter å…³é”®è¯')
    parser.add_argument('--with-summary', action='store_true', dest='with_summary',
                        default=True, help='ä¸ºæ¯æ¡å†…å®¹ç”Ÿæˆä¸­æ–‡æ¦‚è¿°ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-summary', action='store_false', dest='with_summary',
                        help='ç¦ç”¨ä¸­æ–‡æ¦‚è¿°ç”Ÿæˆ')
    parser.add_argument('--date', default=None, 
                        help='æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD æ ¼å¼)ï¼Œé»˜è®¤ä¸ºä»Šå¤©')
    
    args = parser.parse_args()
    
    if args.output is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        args.output = os.path.join(os.path.dirname(script_dir), 'daily_news')
    
    arxiv_items = []
    github_items = []
    hn_items = []
    reddit_items = []
    twitter_items = []
    cg_items = []
    ph_items = []
    skills_items = []
    hf_papers_items = []
    
    # ä¼˜å…ˆä½¿ç”¨ CLI å‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®ï¼Œæœ€åé»˜è®¤
    if args.categories:
        categories = args.categories.split(',')
    else:
        categories = GLOBAL_CONFIG.get('sources', {}).get('arxiv', {}).get('categories', ['cs.AI', 'cs.GR', 'cs.CV'])

    if args.all or args.source == 'arxiv':
        arxiv_items = fetch_arxiv(categories, args.days)
    
    if args.all or args.source == 'github':
        # GitHub é…ç½®
        config = GLOBAL_CONFIG.get('sources', {}).get('github', {})
        # Note: fetch_github needs refactoring too, but for now we pass kwargs or modify it
        # Actually fetch_github calls are simple
        github_items = fetch_github(topics=['graphics', 'ai', 'rendering'])
    
    if args.all or args.source == 'hackernews':
        if args.keywords:
            keywords = args.keywords.split(',')
        else:
            keywords = GLOBAL_CONFIG.get('sources', {}).get('hackernews', {}).get('keywords', ['ai', 'graphics'])
        hn_items = fetch_hackernews(keywords)
    
    if args.all or args.source == 'reddit':
        reddit_items = fetch_reddit()
    
    if args.all or args.source == 'cg':
        cg_items = fetch_cg_graphics()
    
    if args.all or args.source == 'producthunt':
        ph_items = fetch_product_hunt()

    if args.all or args.source == 'trending_skills':
        skills_items = fetch_trending_skills()
    
    if args.all or args.source == 'huggingface':
        hf_papers_items = fetch_huggingface_papers()

    # åˆå¹¶æ‰€æœ‰ items è¿›è¡Œå»é‡
    all_content = []
    all_content.extend(arxiv_items)
    all_content.extend(github_items)
    all_content.extend(hn_items)
    all_content.extend(reddit_items)
    all_content.extend(twitter_items) # Note: twitter_items is alias for bluesky_items
    all_content.extend(cg_items)
    all_content.extend(ph_items)
    all_content.extend(skills_items)
    all_content.extend(hf_papers_items)
    
    # å»é‡
    all_content = deduplicate_items(all_content)
    
    # ç”ŸæˆæŠ¥å‘Š
    has_content = any(all_content)
    if has_content:
        generate_report(
            arxiv_items, 
            github_items, 
            hn_items, 
            str(args.output),
            reddit_items=reddit_items,
            twitter_items=twitter_items,
            cg_items=cg_items,
            ph_items=ph_items,
            trending_skills_items=skills_items,
            hf_items=hf_papers_items,
            with_summary=args.with_summary,
            report_date=args.date
        )
    else:
        print("[WARN] æœªè·å–åˆ°ä»»ä½•å†…å®¹")
        sys.exit(1)



if __name__ == '__main__':
    main()
