#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
[INPUT]: ä¾èµ– httpx, feedparser, beautifulsoup4 è¿›è¡Œç½‘ç»œè¯·æ±‚
[OUTPUT]: å¯¹å¤–æä¾› CLI æ¥å£ï¼Œç”Ÿæˆ Markdown æ ¼å¼çš„æ–°é—»æŠ¥å‘Š
[POS]: ai-cg-news-aggregator Skill çš„æ ¸å¿ƒæ‰§è¡Œè„šæœ¬
[PROTOCOL]: å˜æ›´æ—¶æ›´æ–°æ­¤å¤´éƒ¨ï¼Œç„¶åæ£€æŸ¥ CLAUDE.md
"""

# ============================================================================
#                      AI & CG æ–°é—»èšåˆè„šæœ¬
# ============================================================================

import argparse
import datetime
import json
import os
import re
import sys
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, field

# ============================================================================
#                           æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

@dataclass
class NewsItem:
    """å•æ¡æ–°é—»æ•°æ®"""
    title: str
    url: str
    source: str
    category: str
    score: int = 0
    comments: int = 0
    authors: str = ""
    summary: str = ""
    date: str = ""
    extra: dict = field(default_factory=dict)


# ============================================================================
#                           arXiv æŠ“å–æ¨¡å—
# ============================================================================

def fetch_arxiv(categories: list[str], days: int = 1, max_results: int = 30) -> list[NewsItem]:
    """
    ä» arXiv è·å–æœ€æ–°è®ºæ–‡ï¼ˆä½¿ç”¨ Atom APIï¼Œæ¯” RSS æ›´ç¨³å®šï¼‰
    
    Args:
        categories: arXiv åˆ†ç±»åˆ—è¡¨ï¼Œå¦‚ ['cs.AI', 'cs.GR', 'cs.CV']
        days: è·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
        max_results: æœ€å¤§ç»“æœæ•°
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import feedparser
    except ImportError:
        print("[ERROR] è¯·å®‰è£… feedparser: pip install feedparser")
        return []
    
    items = []
    
    # å…³é”®è¯è¿‡æ»¤ï¼ˆCG å’Œ AI æ ¸å¿ƒè¯é¢˜ï¼‰
    keywords = [
        'neural rendering', 'diffusion', 'transformer', 'ray tracing',
        'real-time', 'GPU', '3D', 'generative', 'NeRF', 'Gaussian',
        'language model', 'vision', 'multimodal', 'embodied'
    ]
    
    per_cat_limit = max(10, max_results // len(categories))
    
    for cat in categories:
        # ä½¿ç”¨ Atom APIï¼ˆæ¯” RSS æ›´ç¨³å®šï¼‰
        url = f"https://export.arxiv.org/api/query?search_query=cat:{cat}&start=0&max_results={per_cat_limit}&sortBy=submittedDate&sortOrder=descending"
        print(f"[INFO] æ­£åœ¨è·å– arXiv {cat}...")
        
        try:
            feed = feedparser.parse(url)
            
            if not feed.entries:
                # é™çº§åˆ° RSS
                rss_url = f"https://export.arxiv.org/rss/{cat}"
                print(f"[INFO] Atom API æ— ç»“æœï¼Œå°è¯• RSS: {cat}")
                feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:per_cat_limit]:
                # æå–æ ‡é¢˜ï¼ˆå»é™¤åˆ†ç±»å‰ç¼€ï¼‰
                title = re.sub(r'^\([^)]+\)\s*', '', entry.title)
                title = title.strip()
                
                # æå–ä½œè€…
                authors = entry.get('author', entry.get('authors', 'Unknown'))
                if isinstance(authors, list):
                    authors = ', '.join([a.get('name', str(a)) for a in authors[:3]])
                
                # æå–æ‘˜è¦ï¼ˆæˆªå–å‰ 200 å­—ç¬¦ï¼‰
                summary = entry.get('summary', '')[:200] + '...'
                
                # æå–é“¾æ¥
                link = entry.get('link', '')
                if isinstance(entry.get('links'), list) and entry.links:
                    for l in entry.links:
                        if l.get('type') == 'application/pdf':
                            link = l.get('href', link)
                            break
                
                # å…³é”®è¯åŒ¹é…è¯„åˆ†
                score = sum(1 for kw in keywords if kw.lower() in title.lower() or kw.lower() in summary.lower())
                
                items.append(NewsItem(
                    title=title,
                    url=link,
                    source='arXiv',
                    category=cat,
                    authors=authors if isinstance(authors, str) else str(authors),
                    summary=summary,
                    score=score,
                    date=entry.get('published', '')
                ))
                
        except Exception as e:
            print(f"[WARN] è·å– {cat} å¤±è´¥: {e}")
    
    # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
    items.sort(key=lambda x: x.score, reverse=True)
    return items[:max_results]


# ============================================================================
#                        GitHub Trending æŠ“å–æ¨¡å—
# ============================================================================

def fetch_github(topics: list[str], language: str = "", since: str = "daily") -> list[NewsItem]:
    """
    ä» GitHub Trending è·å–çƒ­é—¨é¡¹ç›®
    
    Args:
        topics: è¯é¢˜æ ‡ç­¾åˆ—è¡¨
        language: ç¼–ç¨‹è¯­è¨€ç­›é€‰
        since: æ—¶é—´èŒƒå›´ (daily/weekly/monthly)
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import httpx
        from bs4 import BeautifulSoup
    except ImportError:
        print("[ERROR] è¯·å®‰è£…ä¾èµ–: pip install httpx beautifulsoup4")
        return []
    
    items = []
    url = f"https://github.com/trending/{language}?since={since}"
    
    print(f"[INFO] æ­£åœ¨è·å– GitHub Trending ({language or 'all'})...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = httpx.get(url, headers=headers, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è§£æ Trending åˆ—è¡¨
        for article in soup.select('article.Box-row')[:20]:
            
            # é¡¹ç›®åç§°å’Œé“¾æ¥
            h2 = article.select_one('h2 a')
            if not h2:
                continue
            
            repo_path = h2.get('href', '').strip('/')
            repo_url = f"https://github.com/{repo_path}"
            repo_name = repo_path.split('/')[-1] if '/' in repo_path else repo_path
            
            # æè¿°
            desc_elem = article.select_one('p')
            description = desc_elem.get_text(strip=True) if desc_elem else ''
            
            # è¯­è¨€
            lang_elem = article.select_one('[itemprop="programmingLanguage"]')
            lang = lang_elem.get_text(strip=True) if lang_elem else 'Unknown'
            
            # ä»Šæ—¥ Star å¢é‡
            star_elem = article.select_one('.float-sm-right')
            today_stars = 0
            if star_elem:
                star_text = star_elem.get_text(strip=True)
                match = re.search(r'([\d,]+)', star_text)
                if match:
                    today_stars = int(match.group(1).replace(',', ''))
            
            # å…³é”®è¯åŒ¹é…
            keywords = ['graphics', 'rendering', 'ai', 'ml', 'neural', '3d', 'gpu', 'cuda']
            score = sum(1 for kw in keywords if kw in description.lower() or kw in repo_name.lower())
            
            items.append(NewsItem(
                title=repo_name,
                url=repo_url,
                source='GitHub',
                category=lang,
                summary=description[:150],
                score=today_stars + score * 10,
                extra={'today_stars': today_stars, 'language': lang}
            ))
            
    except Exception as e:
        print(f"[WARN] è·å– GitHub Trending å¤±è´¥: {e}")
    
    items.sort(key=lambda x: x.score, reverse=True)
    return items


# ============================================================================
#                        Hacker News æŠ“å–æ¨¡å—
# ============================================================================

def fetch_hackernews(keywords: list[str], min_score: int = 50) -> list[NewsItem]:
    """
    ä» Hacker News è·å–ç›¸å…³çƒ­å¸–
    
    Args:
        keywords: ç­›é€‰å…³é”®è¯
        min_score: æœ€ä½åˆ†æ•°é˜ˆå€¼
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    items = []
    
    print("[INFO] æ­£åœ¨è·å– Hacker News çƒ­å¸–...")
    
    try:
        # è·å– Top Stories
        top_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        response = httpx.get(top_url, timeout=30)
        story_ids = response.json()[:100]  # å–å‰ 100
        
        for story_id in story_ids:
            item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            item_resp = httpx.get(item_url, timeout=10)
            story = item_resp.json()
            
            if not story or story.get('type') != 'story':
                continue
            
            title = story.get('title', '')
            score = story.get('score', 0)
            comments = story.get('descendants', 0)
            
            # åˆ†æ•°è¿‡æ»¤
            if score < min_score:
                continue
            
            # å…³é”®è¯åŒ¹é…
            title_lower = title.lower()
            matched = any(kw.lower() in title_lower for kw in keywords)
            
            if not matched:
                continue
            
            items.append(NewsItem(
                title=title,
                url=story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                source='HackerNews',
                category='Discussion',
                score=score,
                comments=comments,
                extra={'hn_id': story_id}
            ))
            
            # é™åˆ¶è¯·æ±‚é¢‘ç‡
            if len(items) >= 15:
                break
                
    except Exception as e:
        print(f"[WARN] è·å– Hacker News å¤±è´¥: {e}")
    
    return items


# ============================================================================
#                         Reddit æŠ“å–æ¨¡å—
# ============================================================================

def fetch_reddit(
    subreddits: list[str] = None,
    min_upvotes: int = 50,
    limit: int = 15
) -> list[NewsItem]:
    """
    ä» Reddit è·å–çƒ­é—¨å¸–å­
    
    Args:
        subreddits: ç›®æ ‡ Subreddit åˆ—è¡¨
        min_upvotes: æœ€ä½ upvotes é˜ˆå€¼
        limit: æ¯ä¸ª subreddit è·å–æ•°é‡
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    if subreddits is None:
        subreddits = [
            'MachineLearning',
            'GraphicsProgramming',
            'computergraphics',
            'LocalLLaMA',
            'artificial',
            'unrealengine',       # Unreal Engine ç¤¾åŒº
            'unrealengine5',      # UE5 ä¸“å±
            'gamedev'             # æ¸¸æˆå¼€å‘
        ]
    
    items = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AI-CG-NewsBot/1.0'
    }
    
    for sub in subreddits:
        print(f"[INFO] æ­£åœ¨è·å– r/{sub}...")
        url = f"https://www.reddit.com/r/{sub}/hot.json?limit={limit}"
        
        try:
            response = httpx.get(url, headers=headers, timeout=30)
            data = response.json()
            
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post.get('data', {})
                
                # è·³è¿‡ç½®é¡¶å’Œå¹¿å‘Š
                if post_data.get('stickied') or post_data.get('is_self') is None:
                    continue
                
                title = post_data.get('title', '')
                score = post_data.get('ups', 0)
                comments = post_data.get('num_comments', 0)
                permalink = post_data.get('permalink', '')
                
                # è¿‡æ»¤ä½çƒ­åº¦
                if score < min_upvotes:
                    continue
                
                items.append(NewsItem(
                    title=title,
                    url=f"https://www.reddit.com{permalink}",
                    source='Reddit',
                    category=f"r/{sub}",
                    score=score,
                    comments=comments,
                    extra={
                        'subreddit': sub
                    }
                ))
                
        except Exception as e:
            print(f"[WARN] è·å– r/{sub} å¤±è´¥: {e}")
    
    # æ™ºèƒ½æ’åºï¼šç¡®ä¿æ¯ä¸ªç¤¾åŒºè‡³å°‘æœ‰ 1 æ¡ï¼ŒUE ç¤¾åŒºä¼˜å…ˆä¿è¯ 2 æ¡
    # 1. æŒ‰ç¤¾åŒºåˆ†ç»„
    by_subreddit = {}
    for item in items:
        sub = item.extra.get('subreddit', '')
        if sub not in by_subreddit:
            by_subreddit[sub] = []
        by_subreddit[sub].append(item)
    
    # 2. æ¯ä¸ªç¤¾åŒºæŒ‰çƒ­åº¦æ’åº
    for sub in by_subreddit:
        by_subreddit[sub].sort(key=lambda x: x.score, reverse=True)
    
    # 3. ä¼˜å…ˆé€‰å–ï¼šUE ç¤¾åŒºå„å– 2 æ¡ï¼Œå…¶ä»–ç¤¾åŒºå„å– 1 æ¡
    priority_subs = ['unrealengine', 'unrealengine5', 'gamedev']  # ä¼˜å…ˆç¤¾åŒº
    result = []
    
    # ä¼˜å…ˆç¤¾åŒºå„å– 2 æ¡
    for sub in priority_subs:
        if sub in by_subreddit:
            result.extend(by_subreddit[sub][:2])
            by_subreddit[sub] = by_subreddit[sub][2:]
    
    # å…¶ä»–ç¤¾åŒºå„å– 1 æ¡
    for sub, sub_items in by_subreddit.items():
        if sub not in priority_subs and sub_items:
            result.append(sub_items[0])
            by_subreddit[sub] = sub_items[1:]
    
    # 4. å‰©ä½™ä½ç½®æŒ‰çƒ­åº¦å¡«å……
    remaining = []
    for sub_items in by_subreddit.values():
        remaining.extend(sub_items)
    remaining.sort(key=lambda x: x.score, reverse=True)
    
    # å¡«å……åˆ° 20 æ¡
    slots_left = 20 - len(result)
    result.extend(remaining[:slots_left])
    
    # æœ€ç»ˆæŒ‰çƒ­åº¦æ’åºå±•ç¤º
    result.sort(key=lambda x: x.score, reverse=True)
    return result[:20]


# ============================================================================
#                     CG å›¾å½¢å­¦ä¸“å±æŠ“å–æ¨¡å—
# ============================================================================

def fetch_cg_graphics(
    min_upvotes: int = 10,
    limit: int = 15
) -> list[NewsItem]:
    """
    ä»å¤šä¸ª CG å›¾å½¢å­¦ç›¸å…³ç¤¾åŒºè·å–çƒ­é—¨å†…å®¹
    
    è¦†ç›–é¢†åŸŸ:
        - Unreal Engine (r/unrealengine, r/unrealengine5)
        - Three.js (r/threejs)
        - Blender (r/blender, r/blenderhelp)
        - Cinema 4D (r/Cinema4D)
        - Houdini (r/Houdini)
        - ShaderToy / Shaders (r/shaders, r/opengl)
        - é€šç”¨ CG (r/computergraphics, r/GraphicsProgramming)
    
    Args:
        min_upvotes: æœ€ä½ upvotes é˜ˆå€¼
        limit: æ¯ä¸ªç¤¾åŒºè·å–æ•°é‡
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨ï¼ŒAI ç›¸å…³å†…å®¹ä¼˜å…ˆ
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    # CG å›¾å½¢å­¦ä¸“å±ç¤¾åŒºåˆ—è¡¨
    cg_subreddits = [
        # Unreal Engine ç”Ÿæ€
        ('unrealengine', 'Unreal Engine'),
        ('unrealengine5', 'UE5'),
        # Three.js / WebGL
        ('threejs', 'Three.js'),
        # Blender ç”Ÿæ€
        ('blender', 'Blender'),
        ('blenderhelp', 'Blender Help'),
        # Cinema 4D
        ('Cinema4D', 'Cinema 4D'),
        # Houdini
        ('Houdini', 'Houdini'),
        # Shaders / ShaderToy
        ('shaders', 'Shaders'),
        ('opengl', 'OpenGL'),
        # é€šç”¨ CG
        ('computergraphics', 'CG'),
        ('GraphicsProgramming', 'Graphics'),
    ]
    
    # AI ç›¸å…³å…³é”®è¯ï¼ˆç”¨äºä¼˜å…ˆæ’åºï¼‰
    ai_keywords = [
        'ai', 'machine learning', 'ml', 'neural', 'deep learning',
        'gpt', 'llm', 'diffusion', 'stable diffusion', 'midjourney',
        'generative', 'procedural', 'automated', 'artificial intelligence',
        'comfyui', 'automatic', 'dall-e', 'dalle', 'sora', 'kling',
        'nerf', 'gaussian', 'splat', '3d gaussian', 'radiance field'
    ]
    
    items = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) CG-Graphics-Bot/1.0'
    }
    
    for sub, label in cg_subreddits:
        print(f"[INFO] æ­£åœ¨è·å– r/{sub} ({label})...")
        url = f"https://www.reddit.com/r/{sub}/hot.json?limit={limit}"
        
        try:
            response = httpx.get(url, headers=headers, timeout=30)
            data = response.json()
            
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post.get('data', {})
                
                # è·³è¿‡ç½®é¡¶å’Œå¹¿å‘Š
                if post_data.get('stickied'):
                    continue
                
                title = post_data.get('title', '')
                score = post_data.get('ups', 0)
                comments = post_data.get('num_comments', 0)
                permalink = post_data.get('permalink', '')
                selftext = post_data.get('selftext', '')[:200]  # å–å‰200å­—ç¬¦
                
                # è¿‡æ»¤ä½çƒ­åº¦
                if score < min_upvotes:
                    continue
                
                # æ£€æµ‹æ˜¯å¦ AI ç›¸å…³
                text_lower = (title + ' ' + selftext).lower()
                is_ai_related = any(kw in text_lower for kw in ai_keywords)
                
                items.append(NewsItem(
                    title=title,
                    url=f"https://www.reddit.com{permalink}",
                    source='Reddit-CG',
                    category=label,
                    score=score,
                    comments=comments,
                    summary=selftext,
                    extra={
                        'subreddit': sub,
                        'label': label,
                        'is_ai_related': is_ai_related
                    }
                ))
                
        except Exception as e:
            print(f"[WARN] è·å– r/{sub} å¤±è´¥: {e}")
    
    # æ™ºèƒ½æ’åºï¼šAI ç›¸å…³ä¼˜å…ˆï¼Œç„¶åæŒ‰çƒ­åº¦
    # åˆ†ç¦» AI ç›¸å…³å’Œæ™®é€šå†…å®¹
    ai_items = [item for item in items if item.extra.get('is_ai_related')]
    normal_items = [item for item in items if not item.extra.get('is_ai_related')]
    
    # å„è‡ªæŒ‰çƒ­åº¦æ’åº
    ai_items.sort(key=lambda x: x.score, reverse=True)
    normal_items.sort(key=lambda x: x.score, reverse=True)
    
    # AI ç›¸å…³ä¼˜å…ˆï¼Œç„¶åæ™®é€šå†…å®¹
    result = ai_items + normal_items
    
    # ç¡®ä¿æ¯ä¸ªé¢†åŸŸè‡³å°‘æœ‰ 1 æ¡ï¼ˆå¤šæ ·æ€§ä¿è¯ï¼‰
    seen_labels = set()
    diverse_result = []
    remaining = []
    
    for item in result:
        label = item.extra.get('label')
        if label not in seen_labels:
            diverse_result.append(item)
            seen_labels.add(label)
        else:
            remaining.append(item)
    
    # å¡«å……å‰©ä½™ä½ç½®
    diverse_result.extend(remaining)
    
    # åˆå¹¶å®˜æ–¹æºï¼ˆæ›´æƒå¨ï¼‰
    official_items = fetch_cg_official()
    
    # å®˜æ–¹æºæ”¾åœ¨å‰é¢ï¼ˆä¼˜å…ˆå±•ç¤ºï¼‰ï¼Œç„¶åæ˜¯ç¤¾åŒºå†…å®¹
    # ä½†ä»ä¿æŒ AI ç›¸å…³ä¼˜å…ˆ
    all_items = official_items + diverse_result
    
    # é‡æ–°æŒ‰ AI ç›¸å…³æ€§å’Œåˆ†æ•°æ’åº
    ai_items = [item for item in all_items if item.extra.get('is_ai_related')]
    normal_items = [item for item in all_items if not item.extra.get('is_ai_related')]
    
    # å®˜æ–¹æºæ ‡è®° ğŸ›ï¸ï¼Œç¤¾åŒºæºä¿æŒåŸæ ·
    final_result = ai_items + normal_items
    
    return final_result[:30]  # CG ç‰ˆå—è¿”å›æ›´å¤šæ¡ç›®ï¼ˆå«å®˜æ–¹æºï¼‰


# ============================================================================
#                     CG å®˜æ–¹æºæŠ“å–æ¨¡å—
# ============================================================================

def fetch_cg_official() -> list[NewsItem]:
    """
    ä» CG è½¯ä»¶å®˜æ–¹åšå®¢/RSS è·å–æœ€æ–°èµ„è®¯
    
    å®˜æ–¹æºï¼ˆæ›´æƒå¨ã€æ›´å³æ—¶ï¼‰:
        - Unreal Engine å®˜æ–¹åšå®¢
        - Blender å®˜æ–¹åšå®¢
        - Three.js GitHub Releases
        - Houdini (SideFX) å®˜æ–¹åšå®¢
        - Maxon (Cinema 4D) å®˜æ–¹åšå®¢
        - Unity å®˜æ–¹åšå®¢
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    import feedparser
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    # å®˜æ–¹ RSS æºåˆ—è¡¨
    official_feeds = [
        # Unreal Engine
        {
            'name': 'Unreal Engine',
            'url': 'https://www.unrealengine.com/en-US/rss',
            'label': 'UE Official',
            'type': 'rss'
        },
        # Blender
        {
            'name': 'Blender',
            'url': 'https://www.blender.org/feed/',
            'label': 'Blender Official',
            'type': 'rss'
        },
        # Three.js GitHub Releases
        {
            'name': 'Three.js',
            'url': 'https://github.com/mrdoob/three.js/releases.atom',
            'label': 'Three.js Releases',
            'type': 'atom'
        },
        # SideFX Houdini
        {
            'name': 'Houdini',
            'url': 'https://www.sidefx.com/feed/',
            'label': 'Houdini Official',
            'type': 'rss'
        },
        # Unity
        {
            'name': 'Unity',
            'url': 'https://blog.unity.com/feed',
            'label': 'Unity Official',
            'type': 'rss'
        },
        # Godot Engine
        {
            'name': 'Godot',
            'url': 'https://godotengine.org/rss.xml',
            'label': 'Godot Official',
            'type': 'rss'
        },
        # NVIDIA Developer (Graphics)
        {
            'name': 'NVIDIA',
            'url': 'https://developer.nvidia.com/blog/feed/',
            'label': 'NVIDIA Dev',
            'type': 'rss'
        },
    ]
    
    # AI ç›¸å…³å…³é”®è¯
    ai_keywords = [
        'ai', 'machine learning', 'neural', 'deep learning',
        'diffusion', 'generative', 'gaussian', 'nerf', 'dlss',
        'ray tracing', 'rtx', 'tensor'
    ]
    
    items = []
    
    for feed_info in official_feeds:
        name = feed_info['name']
        url = feed_info['url']
        label = feed_info['label']
        
        print(f"[INFO] æ­£åœ¨è·å– {name} å®˜æ–¹æº...")
        
        try:
            feed = feedparser.parse(url)
            
            if not feed.entries:
                print(f"[WARN] {name} æ— æ³•è·å–æˆ–æ— å†…å®¹")
                continue
            
            for entry in feed.entries[:5]:  # æ¯ä¸ªæºå–å‰5æ¡
                title = entry.get('title', '')
                link = entry.get('link', '')
                summary = entry.get('summary', entry.get('description', ''))[:200]
                published = entry.get('published', entry.get('updated', ''))
                
                # æ¸…ç† HTML æ ‡ç­¾
                title = re.sub(r'<[^>]+>', '', title).strip()
                summary = re.sub(r'<[^>]+>', '', summary).strip()
                
                # æ£€æµ‹æ˜¯å¦ AI ç›¸å…³
                text_lower = (title + ' ' + summary).lower()
                is_ai_related = any(kw in text_lower for kw in ai_keywords)
                
                items.append(NewsItem(
                    title=title,
                    url=link,
                    source='Official',
                    category=label,
                    score=100 if is_ai_related else 50,  # AI ç›¸å…³ç»™é«˜åˆ†
                    summary=summary,
                    date=published,
                    extra={
                        'software': name,
                        'label': label,
                        'is_ai_related': is_ai_related,
                        'is_official': True
                    }
                ))
                
        except Exception as e:
            print(f"[WARN] è·å– {name} å®˜æ–¹æºå¤±è´¥: {e}")
    
    # AI ç›¸å…³ä¼˜å…ˆï¼Œç„¶åæŒ‰æ—¥æœŸ/åˆ†æ•°æ’åº
    ai_items = [item for item in items if item.extra.get('is_ai_related')]
    normal_items = [item for item in items if not item.extra.get('is_ai_related')]
    
    ai_items.sort(key=lambda x: x.score, reverse=True)
    normal_items.sort(key=lambda x: x.score, reverse=True)
    
    result = ai_items + normal_items
    
    # ç¡®ä¿æ¯ä¸ªè½¯ä»¶è‡³å°‘æœ‰ 1 æ¡
    seen_software = set()
    diverse_result = []
    remaining = []
    
    for item in result:
        software = item.extra.get('software')
        if software not in seen_software:
            diverse_result.append(item)
            seen_software.add(software)
        else:
            remaining.append(item)
    
    diverse_result.extend(remaining)
    
    return diverse_result[:15]


# ============================================================================
#                        Twitter/X æŠ“å–æ¨¡å— (via Nitter)
# ============================================================================

def fetch_bluesky(
    accounts: list[str] = None,
    keywords: list[str] = None,
    limit: int = 15
) -> list[NewsItem]:
    """
    ä» Bluesky è·å– KOL åŠ¨æ€ï¼ˆä½¿ç”¨ AT Protocol å…¬å¼€ APIï¼‰
    
    Args:
        accounts: Bluesky è´¦å·åˆ—è¡¨ï¼ˆæ ¼å¼ï¼šhandle.bsky.socialï¼‰
        keywords: AI/CG/UE å…³é”®è¯è¿‡æ»¤
        limit: æœ€å¤§è·å–æ•°é‡
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨
    """
    try:
        import httpx
    except ImportError:
        print("[ERROR] è¯·å®‰è£… httpx: pip install httpx")
        return []
    
    if accounts is None:
        # AI/CG/UE é¢†åŸŸæ´»è·ƒè´¦å·
        accounts = [
            'jay.bsky.team',           # Bluesky å®˜æ–¹
            'simonwillison.net',       # AI å¼€å‘è€…
            'stratechery.com',         # ç§‘æŠ€åˆ†æ
            'arstechnica.com',         # ç§‘æŠ€æ–°é—»
        ]
    
    if keywords is None:
        keywords = [
            'ai', 'gpt', 'llm', 'diffusion', 'neural', 'rendering',
            '3d', 'graphics', 'cuda', 'gpu', 'transformer', 'model',
            'paper', 'research', 'release', 'open source',
            'unreal', 'ue5', 'game dev', 'nanite', 'lumen'  # UE ç›¸å…³
        ]
    
    items = []
    headers = {
        'User-Agent': 'AI-CG-NewsBot/1.0'
    }
    
    for account in accounts:
        print(f"[INFO] æ­£åœ¨è·å– @{account} çš„ Bluesky å¸–å­...")
        
        # è§£æ DID
        try:
            resolve_url = f"https://bsky.social/xrpc/com.atproto.identity.resolveHandle?handle={account}"
            resolve_resp = httpx.get(resolve_url, headers=headers, timeout=10)
            
            if resolve_resp.status_code != 200:
                print(f"[WARN] æ— æ³•è§£æ @{account}")
                continue
            
            did = resolve_resp.json().get('did', '')
            if not did:
                continue
            
            # è·å–å¸–å­
            feed_url = f"https://bsky.social/xrpc/app.bsky.feed.getAuthorFeed?actor={did}&limit=20"
            feed_resp = httpx.get(feed_url, headers=headers, timeout=15)
            
            if feed_resp.status_code != 200:
                print(f"[WARN] æ— æ³•è·å– @{account} çš„å¸–å­")
                continue
            
            feed_data = feed_resp.json()
            posts = feed_data.get('feed', [])
            
            for post_item in posts[:10]:
                post = post_item.get('post', {})
                record = post.get('record', {})
                
                text = record.get('text', '')[:200]
                created_at = record.get('createdAt', '')
                uri = post.get('uri', '')
                
                # æ„å»º Bluesky ç½‘é¡µé“¾æ¥
                author = post.get('author', {})
                handle = author.get('handle', '')
                rkey = uri.split('/')[-1] if uri else ''
                web_url = f"https://bsky.app/profile/{handle}/post/{rkey}" if handle and rkey else ''
                
                # å…³é”®è¯åŒ¹é…
                text_lower = text.lower()
                matched = any(kw.lower() in text_lower for kw in keywords)
                
                if matched and text:
                    items.append(NewsItem(
                        title=text,
                        url=web_url,
                        source='Bluesky',
                        category=f"@{account}",
                        date=created_at,
                        extra={
                            'account': account,
                            'handle': handle
                        }
                    ))
                    
        except Exception as e:
            print(f"[WARN] è·å– @{account} å¤±è´¥: {e}")
    
    return items[:limit]


# ä¿ç•™ fetch_twitter ä½œä¸ºåˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
def fetch_twitter(*args, **kwargs):
    """å·²å¼ƒç”¨ï¼šTwitter API ä¸å†å¯ç”¨ï¼Œè¯·ä½¿ç”¨ fetch_bluesky"""
    print("[WARN] Twitter æ•°æ®æºå·²å¼ƒç”¨ï¼ˆAPI å…³é—­ï¼‰ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° Bluesky")
    return fetch_bluesky(*args, **kwargs)


# ============================================================================
#                         ä¸­æ–‡æ¦‚è¿°ç”Ÿæˆæ¨¡å—
# ============================================================================

def generate_chinese_summary(text: str, max_length: int = 80, retries: int = 5) -> str:
    """
    ä¸ºè‹±æ–‡å†…å®¹ç”Ÿæˆä¸­æ–‡æ¦‚è¿°
    
    é‡‡ç”¨åŒé‡ç­–ç•¥:
    1. ç®€å•ç¿»è¯‘ï¼ˆä½¿ç”¨ Google Translate éå®˜æ–¹ APIï¼Œå¸¦é‡è¯•ï¼‰
    2. è‹¥å¤±è´¥åˆ™è¿”å›æˆªæ–­çš„åŸæ–‡å¹¶æ ‡è®° [EN]
    
    Args:
        text: è‹±æ–‡åŸæ–‡
        max_length: æœ€å¤§é•¿åº¦
        retries: é‡è¯•æ¬¡æ•°
    
    Returns:
        ä¸­æ–‡æ¦‚è¿°
    """
    if not text:
        return ""
    
    # æ¸…ç†æ–‡æœ¬
    text = re.sub(r'https?://\S+', '', text)  # ç§»é™¤é“¾æ¥
    text = re.sub(r'\s+', ' ', text).strip()  # åˆå¹¶ç©ºç™½
    original_text = text[:500]  # ä¿ç•™åŸæ–‡ç”¨äº fallback
    
    import time
    import random
    
    # æ¯æ¬¡è°ƒç”¨å‰éšæœºå»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
    time.sleep(random.uniform(0.3, 0.8))
    
    for attempt in range(retries):
        try:
            import httpx
            
            # Google Translate éå®˜æ–¹ API
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'auto',  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                'tl': 'zh-CN',
                'dt': 't',
                'q': original_text
            }
            
            response = httpx.get(url, params=params, timeout=20)
            result = response.json()
            
            # æå–ç¿»è¯‘ç»“æœ
            if result and result[0]:
                translated = ''.join([part[0] for part in result[0] if part[0]])
                # æˆªæ–­åˆ°åˆé€‚é•¿åº¦
                if len(translated) > max_length:
                    translated = translated[:max_length-3] + '...'
                return translated
                
        except Exception as e:
            if attempt < retries - 1:
                wait_time = 1.0 * (attempt + 1) + random.uniform(0.5, 1.5)
                time.sleep(wait_time)  # é€’å¢ç­‰å¾… + éšæœºæŠ–åŠ¨
            else:
                print(f"[WARN] ç¿»è¯‘å¤±è´¥ (é‡è¯• {retries} æ¬¡å): {e}")
    
    # é™çº§ï¼šè¿”å›æˆªæ–­çš„åŸæ–‡ï¼Œæ ‡è®° [EN]
    fallback = original_text[:max_length-8] + '...' if len(original_text) > max_length-5 else original_text
    return f"[EN] {fallback}"


# ============================================================================
#                           æŠ¥å‘Šç”Ÿæˆæ¨¡å—
# ============================================================================

def generate_report(
    arxiv_items: list[NewsItem],
    github_items: list[NewsItem],
    hn_items: list[NewsItem],
    output_dir: str,
    reddit_items: list[NewsItem] = None,
    twitter_items: list[NewsItem] = None,
    cg_items: list[NewsItem] = None,
    with_summary: bool = False,
    report_date: str = None
) -> str:
    """
    ç”Ÿæˆ Markdown æ ¼å¼çš„æ–°é—»æŠ¥å‘Š
    
    Args:
        arxiv_items: arXiv è®ºæ–‡åˆ—è¡¨
        github_items: GitHub é¡¹ç›®åˆ—è¡¨
        hn_items: HN å¸–å­åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        reddit_items: Reddit å¸–å­åˆ—è¡¨
        twitter_items: Twitter æ¨æ–‡åˆ—è¡¨
        cg_items: CG å›¾å½¢å­¦ç›¸å…³æ¡ç›®åˆ—è¡¨
        with_summary: æ˜¯å¦ç”Ÿæˆä¸­æ–‡æ¦‚è¿°
        report_date: æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD æ ¼å¼)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
    
    Returns:
        ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    if report_date:
        today = datetime.datetime.strptime(report_date, '%Y-%m-%d').date()
    else:
        today = datetime.date.today()
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    reddit_items = reddit_items or []
    twitter_items = twitter_items or []
    cg_items = cg_items or []
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    filename = output_path / f"{today.isoformat()}.md"
    
    # æ„å»ºæŠ¥å‘Šå†…å®¹
    lines = [
        f"# ğŸ“° AI & CG æ¯æ—¥èµ„è®¯ - {today.isoformat()}",
        "",
        f"> è‡ªåŠ¨ç”Ÿæˆäº {timestamp}",
        "",
    ]
    
    # GitHub éƒ¨åˆ†
    if github_items:
        lines.extend([
            "## ğŸ”¥ GitHub Trending",
            "",
        ])
        if with_summary:
            lines.extend([
                "| é¡¹ç›® | ä¸­æ–‡æè¿° | è¯­è¨€ | â­ ä»Šæ—¥ | é“¾æ¥ |",
                "|------|----------|------|--------|------|",
            ])
            for item in github_items[:10]:
                desc = generate_chinese_summary(item.summary, 50)
                today_stars = item.extra.get('today_stars', 0)
                lang = item.extra.get('language', 'Unknown')
                lines.append(f"| {item.title} | {desc} | {lang} | +{today_stars} | [Repo]({item.url}) |")
                

        else:
            lines.extend([
                "| é¡¹ç›® | æè¿° | è¯­è¨€ | â­ ä»Šæ—¥ | é“¾æ¥ |",
                "|------|------|------|--------|------|",
            ])
            for item in github_items[:10]:
                desc = item.summary[:40] + '...' if len(item.summary) > 40 else item.summary
                today_stars = item.extra.get('today_stars', 0)
                lang = item.extra.get('language', 'Unknown')
                lines.append(f"| {item.title} | {desc} | {lang} | +{today_stars} | [Repo]({item.url}) |")
                

        lines.append("")
    
    # CG å›¾å½¢å­¦ä¸“å±ç‰ˆå—
    if cg_items:
        lines.extend([
            "## ğŸ¨ CG å›¾å½¢å­¦",
            "",
            "> è¦†ç›–: Unreal Engine | Three.js | Blender | Houdini | Unity | Godot | NVIDIA",
            "> ğŸ›ï¸ = å®˜æ–¹æº | ğŸ¤– = AI ç›¸å…³",
            "",
        ])
        if with_summary:
            lines.extend([
                "| ä¸­æ–‡æ¦‚è¿° | æ¥æº | æ ‡è®° | é“¾æ¥ |",
                "|----------|------|:----:|------|",
            ])
            for item in cg_items[:20]:
                summary = generate_chinese_summary(item.title, 50)
                label = item.category
                # æ ‡è®°ï¼šå®˜æ–¹æº + AI ç›¸å…³
                marks = []
                if item.extra.get('is_official'):
                    marks.append("ğŸ›ï¸")
                if item.extra.get('is_ai_related'):
                    marks.append("ğŸ¤–")
                mark_str = " ".join(marks)
                # é“¾æ¥æ–‡æœ¬
                link_text = "å®˜æ–¹" if item.extra.get('is_official') else "å¸–å­"
                lines.append(f"| {summary} | {label} | {mark_str} | [{link_text}]({item.url}) |")
                

        else:
            lines.extend([
                "| æ ‡é¢˜ | é¢†åŸŸ | AI | çƒ­åº¦ | é“¾æ¥ |",
                "|------|------|:--:|------|------|",
            ])
            for item in cg_items[:15]:
                title = item.title[:50] + '...' if len(item.title) > 50 else item.title
                label = item.category
                is_ai = "ğŸ¤–" if item.extra.get('is_ai_related') else ""
                lines.append(f"| {title} | {label} | {is_ai} | ğŸ”¥ {item.score} | [å¸–å­]({item.url}) |")
                

        lines.append("")
    
    # Bluesky éƒ¨åˆ†ï¼ˆåŸ Twitter/Xï¼‰
    if twitter_items:
        lines.extend([
            "## ğŸ¦‹ Bluesky åŠ¨æ€",
            "",
        ])
        if with_summary:
            lines.extend([
                "| ä¸­æ–‡æ¦‚è¿° | ä½œè€… | é“¾æ¥ |",
                "|----------|------|------|",
            ])
            for item in twitter_items[:10]:
                summary = generate_chinese_summary(item.title, 60)
                lines.append(f"| {summary} | {item.category} | [åŸå¸–]({item.url}) |")
        else:
            lines.extend([
                "| å†…å®¹ | ä½œè€… | é“¾æ¥ |",
                "|------|------|------|",
            ])
            for item in twitter_items[:10]:
                title = item.title[:80] + '...' if len(item.title) > 80 else item.title
                lines.append(f"| {title} | {item.category} | [åŸå¸–]({item.url}) |")
        lines.append("")
    
    # Reddit éƒ¨åˆ†
    if reddit_items:
        lines.extend([
            "## ğŸ”´ Reddit è®¨è®º",
            "",
        ])
        if with_summary:
            lines.extend([
                "| ä¸­æ–‡æ¦‚è¿° | ç¤¾åŒº | çƒ­åº¦ | é“¾æ¥ |",
                "|----------|------|------|------|",
            ])
            for item in reddit_items[:10]:
                summary = generate_chinese_summary(item.title, 60)
                lines.append(f"| {summary} | {item.category} | ğŸ”¥ {item.score} | [å¸–å­]({item.url}) |")
                

        else:
            lines.extend([
                "| æ ‡é¢˜ | ç¤¾åŒº | çƒ­åº¦ | é“¾æ¥ |",
                "|------|------|------|------|",
            ])
            for item in reddit_items[:10]:
                title = item.title[:60] + '...' if len(item.title) > 60 else item.title
                lines.append(f"| {title} | {item.category} | ğŸ”¥ {item.score} | [å¸–å­]({item.url}) |")
                

        lines.append("")
    
    # Hacker News éƒ¨åˆ†
    if hn_items:
        lines.extend([
            "## ğŸ’¬ Hacker News çƒ­è®®",
            "",
        ])
        if with_summary:
            lines.extend([
                "| ä¸­æ–‡æ¦‚è¿° | åˆ†æ•° | è¯„è®º | é“¾æ¥ |",
                "|----------|------|------|------|",
            ])
            for item in hn_items[:10]:
                summary = generate_chinese_summary(item.title, 50)
                lines.append(f"| {summary} | {item.score} | {item.comments} | [è®¨è®º]({item.url}) |")
        else:
            lines.extend([
                "| æ ‡é¢˜ | åˆ†æ•° | è¯„è®º | é“¾æ¥ |",
                "|------|------|------|------|",
            ])
            for item in hn_items[:10]:
                title = item.title[:50] + '...' if len(item.title) > 50 else item.title
                lines.append(f"| {title} | {item.score} | {item.comments} | [è®¨è®º]({item.url}) |")
        lines.append("")
    
    # arXiv å­¦æœ¯å‰æ²¿ï¼ˆæ”¾åœ¨æœ€åï¼‰
    if arxiv_items:
        lines.extend([
            "## ğŸ“ å­¦æœ¯å‰æ²¿ (arXiv)",
            "",
        ])
        
        # æŒ‰åˆ†ç±»åˆ†ç»„
        categories = {}
        for item in arxiv_items:
            cat = item.category
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(item)
        
        for cat, cat_items in categories.items():
            if with_summary:
                lines.extend([
                    f"### {cat}",
                    "",
                    "| ä¸­æ–‡æ¦‚è¿° | ä½œè€… | é“¾æ¥ |",
                    "|----------|------|------|",
                ])
                for item in cat_items[:5]:
                    summary = generate_chinese_summary(item.title)
                    authors = item.authors[:25] + '...' if len(item.authors) > 25 else item.authors
                    lines.append(f"| {summary} | {authors} | [PDF]({item.url}) |")
            else:
                lines.extend([
                    f"### {cat}",
                    "",
                    "| æ ‡é¢˜ | ä½œè€… | é“¾æ¥ |",
                    "|------|------|------|",
                ])
                for item in cat_items[:5]:
                    title = item.title[:60] + '...' if len(item.title) > 60 else item.title
                    authors = item.authors[:30] + '...' if len(item.authors) > 30 else item.authors
                    lines.append(f"| {title} | {authors} | [PDF]({item.url}) |")
            lines.append("")
    
    # é¡µè„š
    lines.extend([
        "---",
        "*æœ¬æŠ¥å‘Šç”± AI & CG News Aggregator Skill è‡ªåŠ¨ç”Ÿæˆ*",
    ])
    
    # å†™å…¥æ–‡ä»¶
    content = '\n'.join(lines)
    filename.write_text(content, encoding='utf-8')
    
    print(f"[OK] æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    return str(filename)



# ============================================================================
#                              ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='AI & CG æ–°é—»èšåˆè„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python fetch_news.py --all                    # è·å–æ‰€æœ‰æ¥æº
  python fetch_news.py --source arxiv           # ä»…è·å– arXiv
  python fetch_news.py --source github          # ä»…è·å– GitHub Trending
  python fetch_news.py --source hackernews      # ä»…è·å– Hacker News
  python fetch_news.py --source reddit          # ä»…è·å– Reddit
  python fetch_news.py --source cg              # ä»…è·å– CG å›¾å½¢å­¦
  python fetch_news.py --source twitter         # ä»…è·å– Twitter/X
  python fetch_news.py --all --with-summary     # å…¨é‡è·å– + ä¸­æ–‡æ¦‚è¿°
  python fetch_news.py --all --output ./news    # æŒ‡å®šè¾“å‡ºç›®å½•
  python fetch_news.py --all --date 2026-01-22  # ç”ŸæˆæŒ‡å®šæ—¥æœŸæŠ¥å‘Š
        """
    )
    
    parser.add_argument('--all', action='store_true', help='è·å–æ‰€æœ‰æ¥æº')
    parser.add_argument('--source', choices=['arxiv', 'github', 'hackernews', 'reddit', 'cg', 'twitter'], 
                        help='æŒ‡å®šå•ä¸€æ¥æº')
    parser.add_argument('--categories', default='cs.AI,cs.GR,cs.CV', help='arXiv åˆ†ç±»ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--days', type=int, default=1, help='è·å–æœ€è¿‘å‡ å¤©çš„å†…å®¹')
    parser.add_argument('--output', default=None, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--keywords', default='ai,graphics,gpu,rendering,neural,llm,diffusion', 
                        help='HN/Twitter å…³é”®è¯')
    parser.add_argument('--with-summary', action='store_true', dest='with_summary',
                        default=True, help='ä¸ºæ¯æ¡å†…å®¹ç”Ÿæˆä¸­æ–‡æ¦‚è¿°ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-summary', action='store_false', dest='with_summary',
                        help='ç¦ç”¨ä¸­æ–‡æ¦‚è¿°ç”Ÿæˆ')
    parser.add_argument('--date', default=None, 
                        help='æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD æ ¼å¼)ï¼Œé»˜è®¤ä¸ºä»Šå¤©')
    
    args = parser.parse_args()
    
    # é»˜è®¤è¾“å‡ºç›®å½•ï¼šç›¸å¯¹äºè„šæœ¬ä½ç½®çš„ ../daily_news/
    if args.output is None:
        script_dir = Path(__file__).parent.resolve()
        args.output = script_dir.parent / 'daily_news'
    
    arxiv_items = []
    github_items = []
    hn_items = []
    reddit_items = []
    twitter_items = []
    cg_items = []
    
    if args.all or args.source == 'arxiv':
        categories = args.categories.split(',')
        arxiv_items = fetch_arxiv(categories, args.days)
    
    if args.all or args.source == 'github':
        github_items = fetch_github(topics=['graphics', 'ai', 'rendering'])
    
    if args.all or args.source == 'hackernews':
        keywords = args.keywords.split(',')
        hn_items = fetch_hackernews(keywords)
    
    if args.all or args.source == 'reddit':
        reddit_items = fetch_reddit()
    
    if args.all or args.source == 'cg':
        cg_items = fetch_cg_graphics()
    
    # Bluesky/Twitter å·²ç¦ç”¨ï¼ˆAPI ä¸ç¨³å®šï¼‰
    # if args.all or args.source == 'twitter':
    #     twitter_items = fetch_twitter()
    
    # ç”ŸæˆæŠ¥å‘Š
    has_content = arxiv_items or github_items or hn_items or reddit_items or twitter_items or cg_items
    if has_content:
        generate_report(
            arxiv_items, 
            github_items, 
            hn_items, 
            str(args.output),
            reddit_items=reddit_items,
            twitter_items=twitter_items,
            cg_items=cg_items,
            with_summary=args.with_summary,
            report_date=args.date
        )
    else:
        print("[WARN] æœªè·å–åˆ°ä»»ä½•å†…å®¹")
        sys.exit(1)



if __name__ == '__main__':
    main()
